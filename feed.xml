<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Michael Stapelbergs Website</title>
  <link href="https://michael.stapelberg.de/feed.xml" rel="self"/>
  <link href="https://michael.stapelberg.de/"/>
  <updated>2017-12-11T10:05:00+01:00</updated>
  <id>https://michael.stapelberg.de/</id>
  <generator>Hugo -- gohugo.io</generator>
  <entry>
    <title type="html"><![CDATA[Docker and IPv6]]></title>
    <link href="https://michael.stapelberg.de/posts/2018-12-12-docker-ipv6/"/>
    <id>https://michael.stapelberg.de/posts/2018-12-12-docker-ipv6/</id>
    <published>2018-12-12T09:54:00+01:00</published>
    <updated>2018-12-12T09:54:00+01:00</updated>
    <content type="html"><![CDATA[

<p>My use-case is seemingly very simple: I want to run a webserver in a Docker
container, and it should be reachable via IPv4 and IPv6. The webserver has
multiple virtual hosts, some of which just serve static files, while others
proxy to, say, a <a href="https://grafana.com">Grafana</a> instance, which is also running
in a Docker container.</p>

<p>This article walks through the required steps, which are a bit cumbersome to
puzzle together from Docker’s official documentation.</p>

<p>I’m using documentation-only IPs (<a href="https://tools.ietf.org/html/rfc3849">RFC3849</a>
and <a href="https://tools.ietf.org/html/rfc5737">RFC5737</a>) throughout the
article. Let’s say that my provider gives me a routed IPv6 subnet
<code>2001:db8:13b:330::/64</code> and the IPv4 address <code>203.0.113.1</code>.</p>

<h3 id="enabling-ipv6-in-docker">Enabling IPv6 in Docker</h3>

<p>The Docker daemon defaults to IPv4-only. To enable IPv6, create the
configuration file <code>/etc/docker/daemon.json</code> with the following content:</p>

<pre><code>{
  &quot;ipv6&quot;: true,
  &quot;fixed-cidr-v6&quot;: &quot;2001:db8:13b:330:ffff::/80&quot;
}
</code></pre>

<p>After restarting the Docker daemon, containers will now get IPv6 addresses based
on their MAC address, which is picked sequentially from the range
<code>02:42:ac:11:00:00</code> to <code>02:42:ac:11:ff:ff</code>. That is, the first container you
start will use the IPv6 address <code>2001:db8:13b:330:ffff:0242:ac11:0002</code>.</p>

<h3 id="publishing-ports-and-remote-addresses">Publishing ports and remote addresses</h3>

<p>When publishing port 80 of a webserver, notice the remote address when accessing
the port via IPv4 and IPv6:</p>

<pre><code>% docker run -p 80:80 nginx
198.51.100.7 - - [12/Dec/2018:07:38:19 +0000] &quot;GET / HTTP/1.1&quot; 200 612
172.17.0.1 - - [12/Dec/2018:07:38:40 +0000] &quot;GET / HTTP/1.1&quot; 200 612
</code></pre>

<p>The first request (IPv4) has the correct remote address, but not the second one
(IPv6). This is because Docker publishes ports with
<a href="https://en.wikipedia.org/wiki/Network_address_translation">NAT</a> for IPv4, and a
TCP proxy for IPv6.</p>

<p>Of course, not having access to the actual remote address breaks rate limiting,
abuse blocking, address-based geo location, etc.</p>

<p>Some people resort to using Docker’s <code>host</code> network option, but that’s not a
good solution: your container will not be able to talk to other containers by
name anymore, so you will need lots of static, host-specific configuration.</p>

<p>A better solution is to only publish the port via IPv4 and connect to the
container’s IPv6 address directly:</p>

<pre><code>% docker run --publish 203.0.113.1:80:80 --name nginx nginx
</code></pre>

<p>You can obtain the container’s IPv6 address using:</p>

<pre><code>% docker inspect -f '{{.NetworkSettings.GlobalIPv6Address}}' nginx
</code></pre>

<h3 id="static-ipv6-addresses">Static IPv6 addresses</h3>

<p>Above, I explained that we need to use the container’s IPv6 address directly,
and that the address is derived from the MAC address, which is chosen
sequentially at container start time.</p>

<p>Having addresses depend on the order in which containers come up isn’t a robust
solution for my simple setup, where I want to statically configure a DNS record.</p>

<p>Docker allows specifying an IPv6 address, but only when you’re using a <a href="https://docs.docker.com/network/network-tutorial-standalone/#use-user-defined-bridge-networks">user-defined bridge network</a> with an IPv6 subnet carved out for the network, like so:</p>

<pre><code>% docker network create --subnet 2001:db8:13b:330:dd::/80 --ipv6 nginx

% docker run \
  --network nginx \
  --ip6 2001:db8:13b:330:dd:ff::1 \
  --publish 203.0.113.1:80:80 \
  nginx
</code></pre>

<p>Note that I’m using an IPv6 address from the far end of the address space
(<code>ff::1</code>), so as to not conflict with the addresses that Docker sequentially
allocates from the network we created.</p>

<p>Now, create a DNS record with the container’s addresses and you’ll be able to
access it via IPv4 and IPv6 with correct remote addresses, while still being
able to reach other containers:</p>

<pre><code>www.example.net A    203.0.113.1
www.example.net AAAA 2001:db8:13b:330:dd:ff::1
</code></pre>

<p>Note that all other Docker containers that you want to reach from the nginx
container must also use the nginx network. This is the recommended solution over
the old <code>--link</code> flag anyway.</p>

<p>One disadvantage of this solution is that you cannot offer services from
multiple Docker containers on the same IPv6 address (e.g. www and git).</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Network setup for our retro computing event RGB2Rv18]]></title>
    <link href="https://michael.stapelberg.de/posts/2018-10-31-rgb2rv18-network/"/>
    <id>https://michael.stapelberg.de/posts/2018-10-31-rgb2rv18-network/</id>
    <published>2018-10-30T23:00:00+01:00</published>
    <updated>2018-10-30T23:00:00+01:00</updated>
    <content type="html"><![CDATA[

<p>Our computer association <a href="https://www.noname-ev.de/">NoName e.V.</a> organizes a
retro computing event called <a href="https://www.rgb2r.de/">RGB2R</a> every year,
located in Heidelberg, Germany. This year’s version is called RGB2Rv18.</p>

<p>This article describes the network setup I created for this year’s event. If you
haven’t read it, the article about <a href="/posts/2017-11-13-rgb2r-network/">last year’s RGB2Rv17
network</a> is also available.</p>

<h3 id="connectivity">Connectivity</h3>

<p>As a reminder, the venue’s DSL connection tops out at a megabit or two, so we
used my parent’s 400 Mbit/s cable internet line, like last year.</p>

<p>A difference to last year is that we switched from the tp-link CPE510 devices to
a pair of <a href="https://www.ubnt.com/airfiber/airfiber24/">Ubiquiti airFiber24</a>. The
airFibers are specified to reach 1.4 Gbit/s. In practice, we reached
approximately 700 Mbps displayed capacity (at a signal strength of ≈-60 dBm) and
422 Mbit/s end-to-end download speed, limited by the cable uplink.</p>

<p>Notably, using a single pair of radios removes a bunch of complexity from the
network setup as we no longer need to load-balance over two separate uplinks.</p>

<p>Like last year, the edge router for the event venue was a <a href="https://pcengines.ch/apu2c4.htm">PC Engines
apu2c4</a>. For the Local Area Network (LAN)
within the venue, we provided a few switches and WiFi using <a href="https://www.ubnt.com/">Ubiquiti
Networks</a> access points.</p>

<h3 id="wifi-setup">WiFi setup</h3>

<p>It turns out that the 24 GHz-based airFiber radios are much harder to align than
the 5 GHz-based tp-links we used last year. With the tp-link devices, we were
able to easily obtain a link, and do maybe 45 minutes of fine tuning to achieve
maximum bandwidth.</p>

<p>With the airFiber radios mounted in the same location, we were unable to
establish a link even once in about 1.5 hours of trying. We think this was due
to trees/branches being in the way, so we decided to scout the property for a
better radio location with as much of a direct line of sight as possible.</p>

<p>We eventually found a better location on the other side of the house and managed
to establish a link. It still took us an hour or so of fine tuning to move the
link from weak (≈-80 dBm) to okay (≈-60 dBm).</p>

<p>After the first night, in which it rained for a while, the radios had lost their
link. We think that this might be due to the humidity, and managed to restore
the link after another 30 minutes of re-adjustment.</p>

<p>It also rained the second night, but this time, the link stayed up. During rain,
signal strength dropped from ≈-60 dBm to ≈-72 dBm, but that still resulted in
≈500 Mbit/s of WiFi capacity, sufficient to max out our uplink.</p>

<p>For next year, it would be great to use an antenna alignment tool of sorts to
cut down on setup time. Alternatively, we could switch to more forgiving radios
which also handle 500 Mbps+. Let me know if you have any suggestions!</p>

<h3 id="software">Software</h3>

<p>In May this year, I wrote <a href="https://github.com/rtr7/router7">router7</a>, a pure-Go
small home internet router. Mostly out of curiosity, we gave it a shot, and I’m
happy to announce that router7 ran the event without any trouble.</p>

<p>In preparation, I <a href="https://github.com/rtr7/router7/commit/2e8e0daa0ac8a6a123893b27fb1de566768383d0">implemented TCP MSS
clamping</a>
and <a href="https://github.com/rtr7/kernel/commit/c7afbc1fd2efdb9e1149d271c4d2be59cc5c98f4">included the WireGuard kernel
module</a>.</p>

<p>I largely followed the <a href="https://github.com/rtr7/router7#installation">router7 installation
instructions</a>. To be specific,
here is the <code>Makefile</code> I used for creating the router7 image:</p>

<pre><code># github.com/rtr7/router7/cmd/... without dhcp6,
# as the cable uplink does not provide IPv6:
PKGS := github.com/rtr7/router7/cmd/backupd \
	github.com/rtr7/router7/cmd/captured \
	github.com/rtr7/router7/cmd/dhcp4 \
	github.com/rtr7/router7/cmd/dhcp4d \
	github.com/rtr7/router7/cmd/diagd \
	github.com/rtr7/router7/cmd/dnsd \
	github.com/rtr7/router7/cmd/netconfigd \
	github.com/rtr7/router7/cmd/radvd \
	github.com/gokrazy/breakglass \
	github.com/gokrazy/timestamps \
	github.com/stapelberg/rgb2r/cmd/grafana \
	github.com/stapelberg/rgb2r/cmd/prometheus \
	github.com/stapelberg/rgb2r/cmd/node_exporter \
	github.com/stapelberg/rgb2r/cmd/blackbox_exporter \
	github.com/stapelberg/rgb2r/cmd/ratelimit \
	github.com/stapelberg/rgb2r/cmd/tc \
	github.com/stapelberg/rgb2r/cmd/wg

image:
ifndef DIR
	@echo variable DIR unset
	false
endif
	GOARCH=amd64 gokr-packer \
		-gokrazy_pkgs=github.com/gokrazy/gokrazy/cmd/ntp,github.com/gokrazy/gokrazy/cmd/randomd \
		-kernel_package=github.com/rtr7/kernel \
		-firmware_package=github.com/rtr7/kernel \
		-overwrite_boot=${DIR}/boot.img \
		-overwrite_root=${DIR}/root.img \
		-overwrite_mbr=${DIR}/mbr.img \
		-serial_console=ttyS0,115200n8 \
		-hostname=rgb2router \
		${PKGS}
</code></pre>

<p>After preparing an <code>interfaces.json</code> configuration file and a
<a href="https://github.com/gokrazy/breakglass">breakglass</a> SSH hostkey, I used
<code>rtr7-recover</code> to net-install the image onto the apu2c4. For subsequent updates,
I used <code>rtr7-safe-update</code>.</p>

<p>The Go packages under <code>github.com/stapelberg/rgb2r</code> are wrappers which run
software I installed to the permanent partition mounted at <code>/perm</code>. See
<a href="https://gokrazy.org/prototyping.html">gokrazy: Prototyping</a> for more details.</p>

<h3 id="tunnel-setup">Tunnel setup</h3>

<p>Last year, we used a Foo-over-UDP tunnel after noticing that we didn’t get
enough bandwidth with OpenVPN. This year, after hearing much about it, we
successfully used <a href="https://www.wireguard.com/">WireGuard</a>.</p>

<p>I found WireGuard to be more performant than OpenVPN, and easier to set up than
either OpenVPN or Foo-over-UDP.</p>

<p>The one wrinkle is that its wire protocol is not yet frozen, and its kernel
module is not yet included in Linux.</p>

<h3 id="traffic-shaping">Traffic shaping</h3>

<p>With asymmetric internet connections, such as the 400/20 cable connection we’re
using, it’s necessary to shape traffic such that the upstream is never entirely
saturated, otherwise the TCP ACK packets won’t reach their destination in time
to saturate the downstream.</p>

<p>While the FritzBox might already provide traffic shaping, we wanted to
voluntarily restrict our upstream usage to leave some headroom for my parents.</p>

<pre><code>rgb2router# tc qdisc replace dev uplink0 root tbf \
  rate 16mbit \
  latency 50ms \
  burst 4000
</code></pre>

<p>The specified <code>latency</code> value is a best guess, and the <code>burst</code> value is derived
from the kernel internal timer frequency (<code>CONFIG_HZ</code>) (!), packet size and rate
as per
<a href="https://unix.stackexchange.com/questions/100785/bucket-size-in-tbf">https://unix.stackexchange.com/questions/100785/bucket-size-in-tbf</a>.</p>

<p>Tip: keep in mind to disable shaping temporarily when you’re doing bandwidth
tests ;-).</p>

<h3 id="statistics">Statistics</h3>

<ul>
<li><p>We peaked at 59 active DHCP leases, which is very similar to the “about 60”
last year.</p></li>

<li><p>DNS traffic peaked at about 25 queries/second, while mostly remaining at less
than 5 queries/second.</p></li>

<li><p>We were able to obtain peaks of nearly 200 Mbit/s of download traffic and
transferred over 200 GB of data, twice as much as last year.</p></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Installing Dell’s Ubuntu image on an XPS 9360]]></title>
    <link href="https://michael.stapelberg.de/posts/2018-05-07-xps-13-9360-linux/"/>
    <id>https://michael.stapelberg.de/posts/2018-05-07-xps-13-9360-linux/</id>
    <published>2018-05-07T09:10:00+02:00</published>
    <updated>2018-05-07T09:10:00+02:00</updated>
    <content type="html"><![CDATA[

<p>Warning: if you don’t understand one of the steps, don’t blindly follow them,
but ask a friend for help instead! Be sure to have a known-working backup
before messing with your system.</p>

<h3 id="motivation">Motivation</h3>

<p>I recently got a Dell XPS 13 9360 (2017), which of course I would like to use
with Linux. I figured I’d give Dell’s Ubuntu version a try, as it is the closest
I can get to a supported Linux offering on a modern laptop.</p>

<p>Unfortunately, Dell doesn’t sell the XPS 9360 in its shop anymore (and I don’t
like its successor due to the lack of USB A ports), so I had to resort to buying
a version that comes with Windows.</p>

<p>You can obtain the recovery image
<code>dell-bto-xenial-dino2-mlk-A00-iso-20161021-0.iso</code> from
<a href="http://www.dell.com/support/home/us/en/19/Drivers/OSISO/W764">http://www.dell.com/support/home/us/en/19/Drivers/OSISO/W764</a>, provided you have
the system tag of an XPS 9360 that came with Linux — ask a friend if you
accidentally purchased the Windows version. Tip: the service tag is the BIOS
serial number, which is included in the output of
<a href="https://manpages.debian.org/stretch/lshw/lshw.1.en"><code>lshw(1)</code></a> and similar
tools.</p>

<h3 id="making-the-image-bootable">Making the image bootable</h3>

<p>I don’t understand why this image is not bootable by default. The device it was
generated for never had a CD/DVD drive, so what good is using an ISO 9660 image?</p>

<p>Anyway, to make the image bootable, I formatted a USB thumb drive with a FAT
file system and installed GRUB in such a way that it will loopback-boot into the
ISO image (this is option 2 from <a href="https://askubuntu.com/a/395880">https://askubuntu.com/a/395880</a>):</p>

<pre><code>grub-mkimage -o bootx64.efi -p /efi/boot -O x86_64-efi \
 fat iso9660 part_gpt part_msdos \
 normal boot linux configfile loopback chain \
 efifwsetup efi_gop efi_uga \
 ls search search_label search_fs_uuid search_fs_file \
 gfxterm gfxterm_background gfxterm_menu test all_video loadenv \
 exfat ext2 ntfs btrfs hfsplus udf
cat &gt;grub.cfg &lt;&lt;'EOT'
set timeout=3
set color_highlight=black/light-magenta

menuentry 'Ubuntu ISO' {
        set isofile=&quot;/efi/boot/dell-bto-xenial-dino2-mlk-A00-iso-20161021-0.iso&quot;
        loopback loop $isofile
        linux (loop)/casper/vmlinuz.efi boot=casper iso-scan/filename=$isofile noprompt noeject quiet splash persistent --
        initrd (loop)/casper/initrd.lz
}
EOT

sudo mkfs.vfat /dev/sdc
pmount /dev/sdc
mkdir -p /media/sdc/efi/boot
cp bootx64.efi *.iso grub.cfg /media/sdc/efi/boot
pumount sdc
</code></pre>

<h3 id="making-the-installer-work">Making the installer work</h3>

<p>To get the installer to work, I had to comment out the <code>self.genuine</code> checks in
<code>/usr/lib/ubiquity/plugins/dell-{bootstrap,recovery}.py</code>, then start <code>ubiquity</code>
from the terminal.</p>

<h3 id="switch-to-luks-full-disk-encryption">Switch to LUKS full-disk encryption</h3>

<p>Unfortunately, the recovery installation only offers you to encrypt your
homedir, whereas I would like to encrypt my entire disk. Here are the steps I
took to enable LUKS full-disk encryption.</p>

<p>First, I copied the root file system’s data to my backup storage. Pay attention
to copying the files as root, otherwise setuid and setgid bits might get lost.</p>

<pre><code># mount /dev/mapper/nvme0n1p3 /mnt/root
# rsync -aXx --relative --numeric-ids /mnt/root/ root@storage:xps/
# umount /mnt/root
</code></pre>

<p>Then, I created a new boot partition, encrypted the root partition and re-created the file system:</p>

<pre><code># cat &lt;&lt;'EOT' | sfdisk /dev/nvme0n1
label: gpt
label-id: C64A87D1-CA61-4BF2-81E6-0216EE6BC4C0
device: /dev/nvme0n1
unit: sectors
first-lba: 34
last-lba: 2000409230

/dev/nvme0n1p1 : start=          34, size=      488248, type=C12A7328-F81F-11D2-BA4B-00A0C93EC93B, name=&quot;EFI System Partition&quot;
/dev/nvme0n1p2 : start=      488282, size=     5380681, type=EBD0A0A2-B9E5-4433-87C0-68B6B72699C7, name=&quot;fat32&quot;
/dev/nvme0n1p3 : start=     5869568, size=     2097152, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4
/dev/nvme0n1p4 : start=     7966720, size=  1992442511, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4
EOT
# mkfs.ext2 /dev/nvme0n1p3
# cryptsetup luksFormat /dev/nvme0n1p4
# cryptsetup luksOpen /dev/nvme0n1p4 nvme0n1p3_crypt
# mkfs.ext4 /dev/mapper/nvme0n1p4_crypt
</code></pre>

<p>Next, I restored the data:</p>

<pre><code># mount /dev/mapper/nvme0n1p4_crypt /mnt/root
# mount /dev/nvme0n1p3 /mnt/root/boot
# mkdir /mnt/root/boot/efi
# mount /dev/nvme0n1p1 /mnt/root/boot/efi
# rsync -aXx --numeric-ids storage:xps/ /mnt/root
</code></pre>

<p>And finally, I fixed the boot partition:</p>

<pre><code># mount -o bind /dev /mnt/root/dev
# mount -t sysfs sysfs /mnt/root/sys
# mount -t proc proc /mnt/root/proc
# chroot /mnt/root /bin/bash

# apt install cryptsetup
# echo nvme0n1p4_crypt UUID=$(blkid -o value -s UUID /dev/nvme0n1p4) none luks,discard &gt; /etc/crypttab
# cat &gt;/etc/fstab &lt;&lt;EOT
UUID=$(blkid -o value -s UUID /dev/mapper/nvme0n1p4_crypt) / ext4 errors=remount-ro 0 1
UUID=$(blkid -o value -s UUID /dev/nvme0n1p3) /boot ext2 defaults 0 0
UUID=$(blkid -o value -s UUID /dev/nvme0n1p1) /boot/efi vfat umask=0077 0 1
EOT
# update-initramfs -u
# update-grub
# grub-install

# umount /mnt/root/{proc,sys,dev,boot/efi,boot,}
</code></pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[kinX: USB Hub]]></title>
    <link href="https://michael.stapelberg.de/posts/2018-04-17-kinx-usb-hub/"/>
    <id>https://michael.stapelberg.de/posts/2018-04-17-kinx-usb-hub/</id>
    <published>2018-04-17T17:49:00+02:00</published>
    <updated>2018-04-17T17:49:00+02:00</updated>
    <content type="html"><![CDATA[

<p>This post is part of a <a href="/posts/2018-04-17-kinx">series of posts about the kinX project</a>.</p>

<h3 id="motivation">Motivation</h3>

<p>The Kinesis Advantage comes with a built-in 2-port USB hub. That hub uses a
proprietary connector to interface with a
<a href="https://en.wikipedia.org/wiki/PS/2_port">PS/2</a> keyboard controller, so it
cannot be used with a USB keyboard controller. As the built-in hub is the
natural place to connect the Logitech unified receiver dongle, not being able to
use the hub is mildly annoying.</p>

<p>The kinX MK66F keyboard controller presently needs two USB cables: one connected
to the USBFS port to supply the PCB with power and receive firmware updates (via
the Teensy bootloader chip), and one connected to the USBHS port for the actual
keyboard device.</p>

<p>Lastly, even if the original built-in USB hub had internal ports (instead of a
PS/2 converter), it only supports USB 1.1, nullifying any latency improvements.</p>

<p>Hence, I decided to build a drop-in replacement USB 2.0 hub with 2 external USB
ports and 2 internal USB ports, using the same proprietary connector as the
original, so that the original keyboard USB cable could be re-used.</p>

<h3 id="design-phase">Design phase</h3>

<p>Unfortunately, I could not find an open hardware USB 2.0 hub design on the
internet, so I started researching various USB hub chips. I quickly discarded
the idea of using USB 3 due to its much stricter requirements.</p>

<p>In the end, I decided to go with the Cypress HX2VL series because of their
superior documentation: I found a detailed data sheet, an evaluation board, the
associated schematics, design checklist/guidelines, and even the evaluation
board’s bill of materials.</p>

<p>This is what the finished build of my design looks like:</p>

<p><img src="/Bilder/kinx-hub.jpg" width="100%"></p>

<h3 id="power">Power</h3>

<p>After completing my first build, I tested a few USB devices with my hub. The
Logitech unified receiver dongle and the
<a href="https://www.yubico.com/start/">YubiKey</a> worked fine. However, my external hard
drive and my USB memory stick did not work. In the syslog, I would see:</p>

<pre><code>kernel: usb 1-14.4.4: rejected 1 configuration due to insufficient available bus power
</code></pre>

<p>This is because the USB specification limits bus-powered hubs to 100mA per
port. While high power usage does not come as a surprise for the external hard
disk, it turns out that even my USB memory stick requires 200mA. This was a
surprise, because that stick works on other, commercial bus-powered USB hubs.</p>

<p>A closer look reveals that all 3 commercial USB hubs I have tested claim to be
self-powered (i.e. using an external power supply), even though they are
not. This way, the kernel’s power limitation is circumvented, and up to 500mA
can be used per port. In practice, the host port only supplies 500mA, so the
user must be careful not to plug in devices which require more than 500mA in
aggregate.</p>

<p>I changed the SELFPWR configuration pin to have my hub claim it was
self-powered, too, and that made all USB devices I tested work fine.</p>

<h3 id="eeprom-programming">EEPROM programming</h3>

<p>When debugging the power issue, I originally thought the Maximum Power setting
in the hub’s USB device descriptor needed to be raised. This turned out to not
be correct: the Maximum Power refers to the power which the hub uses for its own
circuitry, not the power it passes through to connected devices.</p>

<p>Nevertheless, it’s a nice touch to modify the device descriptor to put in a
custom vendor name, product name and serial number: that way, the device shows
up with a recognizable name in your syslog or
<a href="https://manpages.debian.org/stretch/usbutils/lsusb.8"><code>lsusb(8)</code></a> output, and
udev rules can be used to apply settings based on the serial number.</p>

<p>To modify the device descriptor, an <a href="https://en.wikipedia.org/wiki/EEPROM">EEPROM (electrically erasable programmable
read-only memory)</a> needs to be added to
the design, from which the HX2VL will read configuration.</p>

<p>The HX2VL allows field-programming of the connected EEPROM, i.e. writing to it
via the USB hub chip. I found the Windows-only tool hard to set up on a modern
Windows installation, so I wondered whether I could build a simpler to use tool.</p>

<p>Under the covers, the tool merely sends commands with the vendor-specific
request code 14 via USB, specifying an index of the two-byte word to
read/write. This can be replicated in a few lines of Go:</p>

<pre><code>dev, _ := usb.OpenDeviceWithVIDPID(0x04b4, 0x6570)
eepromRequest := 14
wIndex := 0 // [0, 63] for 128 bytes of EEPROM
dev.Control(gousb.RequestTypeVendor|0x80, 
  eepromRequest, 0, wIndex, make([]byte, 2))
</code></pre>

<p>The EEPROM contents are well-described in the <a href="http://www.cypress.com/file/114101/download">HX2VL data
sheet</a>, so the rest is easy.</p>

<p>See <a href="https://github.com/kinx-project/mk66f-blaster">https://github.com/kinx-project/mk66f-blaster</a> for the tool.</p>

<h3 id="lessons-learnt">Lessons learnt</h3>

<ul>
<li><p>If possible, design the PCB in such a way that components you think you don’t
need (e.g. the EEPROM) can optionally be soldered on. This would have saved me
a PCB design/fabrication cycle.</p></li>

<li><p>Get the evaluation board to figure out the configuration you need
(e.g. self-powered vs. bus-powered).</p></li>
</ul>

<h3 id="next-up">Next up</h3>

<p>The <a href="/posts/2018-04-17-kinx-latency-measurement/">last post introduces the processing latency measurement firmware for the
FRDM-K66F development board</a> and
draws a conclusion.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[kinX: keyboard controller with &lt;0.225ms input latency]]></title>
    <link href="https://michael.stapelberg.de/posts/2018-04-17-kinx-keyboard-controller/"/>
    <id>https://michael.stapelberg.de/posts/2018-04-17-kinx-keyboard-controller/</id>
    <published>2018-04-17T17:49:00+02:00</published>
    <updated>2018-04-17T17:49:00+02:00</updated>
    <content type="html"><![CDATA[

<p>This post is part of a <a href="/posts/2018-04-17-kinx">series of posts about the kinX project</a>.</p>

<h3 id="background">Background</h3>

<p>10 years ago I got a Kinesis Advantage keyboard. I wrote about the experience of
learning to touch-type using the ergonomic <a href="https://neo-layout.org/">NEO layout</a>
in my (German) post <a href="/posts/2009-01-01-neo_kinesis/">“Neo-Layout auf einer
Kinesis-Tastatur”</a>.</p>

<p>The Kinesis Advantage is still the best keyboard I’ve ever used, and I use one
every day, both at the office and at home.</p>

<p>I had two reasons to start modifying the keyboard:</p>

<ol>
<li><p>I prefer Cherry MX blue key switches over the Cherry MX brown key switches
the Kinesis comes with. Nowadays, you can get a Kinesis with Cherry MX red
key switches, which felt okay in a quick test.</p></li>

<li><p>The original keyboard controller has (had?) a bug where modifier keys such as
Shift would get stuck at least once a week: you would press Shift, press A,
release A, release Shift, press A and see AA instead of Aa.</p></li>
</ol>

<p>I solved issue ① with the help of the excellent Kinesis technical support, who
sold me unpopulated PCBs so that I could solder on my own key switches.</p>

<p>Issue ② was what lead to my first own keyboard controller build, which I
documented in <a href="/posts/2013-03-21-kinesis_custom_controller/">“Hacking your own Kinesis keyboard
controller”</a> (2013).</p>

<p>Then, the topic of input latency popped into my filter bubble, with excellent
 posts such as <a href="https://pavelfatin.com/typing-with-pleasure/">Pavel Fatin’s “Typing with
pleasure”</a>. I started wondering
what input latency I was facing, and whether/how I could reduce it.</p>

<p>Given that I was using a custom keyboard controller, it was up to me to answer
that question. After trying to understand and modify the firmware I had been
using for the last 4 years, I realized that I first needed to learn much more
about how keyboards work.</p>

<p>I firmly believe that creating a good environment is vital for development,
especially for intrinsically-motivated side projects like this one. Hence, I set
the project aside until a colleague gifted me his old Kinesis which had
intermittent issues. I removed the electronics and started using that keyboard
as my development keyboard.</p>

<h3 id="sources-of-input-latency">Sources of input latency</h3>

<p>A keyboard controller has 3 major tasks:</p>

<ul>
<li><p><strong>matrix scan</strong>: to avoid physically connecting every single key switch
directly to a microcontroller (requiring a large number of GPIO pins), most
keyboards use a matrix. See <a href="http://blog.komar.be/how-to-make-a-keyboard-the-matrix/">“How to make a keyboard — the
matrix”</a> for a good
explanation.</p></li>

<li><p><strong>debouncing</strong>: when pressing a key switch, it doesn’t cleanly change from a
low voltage level to a high voltage level (or vice-versa). Instead, it
bounces: the voltage level rapidly oscillates until it eventually reaches a
stable steady state. Because one key press shouldn’t result in a whole bunch
of characters, keyboard controllers need to debounce the key press.</p></li>

<li><p><strong>USB</strong>: nowadays, keyboards use USB (for example to be compatible with
laptops, which generally don’t have PS/2 ports), so the keyboard’s state needs
to be communicated to the computer via USB.</p></li>
</ul>

<p>Here’s an illustration of the timing of a key press being handled by a naive
keyboard controller implementation:</p>

<p><img src="/Bilder/kinx-input-latency-sources.svg"></p>

<p>In the worst case, a key press happens just after a keyboard matrix scan. The
first source of latency is the time it takes until the next keyboard matrix scan
happens.</p>

<p>Depending on the implementation, the key press now sits in a data structure,
waiting for the debounce time to pass.</p>

<p>Finally, once the key press was successfully debounced, the device must wait
until the USB host polls it before it can send the HID report.</p>

<p>Unless the matrix scan interval is coupled to the USB poll interval, the delays
are additive, and the debounce time is usually constant: in the best case, a key
press happens just before a matrix scan (0ms) and gets debounced (say, 5ms) just
before a USB poll (0ms).</p>

<h3 id="teensy-3-6-controller-for-learning">Teensy 3.6 controller (for learning)</h3>

<p>My old keyboard controller used the
<a href="https://www.pjrc.com/teensy/index.html">Teensy++</a>, which is fairly dated at
this point. I decided a good start of the project would be to upgrade to the
current Teensy 3.6, cleaning up the schematics on the way.</p>

<p><img src="/Bilder/kinx-teensy36.jpg" width="100%"></p>

<p>To ensure I understand all involved parts, I implemented a bare-metal firmware
almost from scratch: I cobbled together the required startup code, USB stack
and, most importantly, key matrix scanning code.</p>

<p>In my firmware, the Teensy 3.6 runs at 180 MHz (compared to the Teensy++’s 16
MHz) and scans the keyboard matrix in a busy loop (as opposed to on USB
poll). Measurements confirmed a matrix scan time of only 100μs (0.1ms).</p>

<p>I implemented debouncing the way it is described in <a href="https://summivox.wordpress.com/2016/06/03/keyboard-matrix-scanning-and-debouncing/">Yin Zhong’s “Keyboard
Matrix Scanning and
Debouncing”</a>:
by registering a key press/release on the rising/falling edge and applying the
debounce time afterwards, effectively eliminating debounce latency.</p>

<p>Note that while the Cherry MX datasheet specifies a debounce time of 5ms, I
found it necessary to increase the time to 10ms to prevent bouncing in some of
my key switches, which are already a few years old.</p>

<p>I set the USB device descriptor’s poll interval to 1, meaning poll every 1 USB
micro frame, which is 1ms long with USB 1.x (Full Speed).</p>

<p>This leaves us at an input latency within [0ms, 1.1ms]:</p>

<ul>
<li>≤ 0.1ms scan latency</li>
<li>0ms debounce latency</li>
<li>≤ 1ms USB poll latency</li>
</ul>

<p>Can we reduce the input latency even further? The biggest factor is the USB poll
interval.</p>

<h3 id="usb-high-speed">USB High Speed</h3>

<p>With USB 2.0 High Speed, the micro frame duration is reduced to 125μs
(0.125ms). The NXP MK66F micro controller in the Teensy 3.6 has two USB ports:</p>

<ol>
<li>the Full Speed-only USBFS port, which is used by the Teensy 3.6</li>
<li>the High Speed-capable USBHS port, which the Teensy optionally uses for host
mode, with experimental software support (at the time of writing)</li>
</ol>

<p><img src="/Bilder/kinx-usbhs-breakout.jpg" width="100%"></p>

<p>While the software support was a road block which could conceivably be solved, I
also faced a mechanical problem: the available space in the Kinesis keyboard and
the position of the USB High Speed port pins on the Teensy 3.6 unfortunately
prevented installing any sort of breakout board to actually use the port.</p>

<p>I decided to move from the Teensy 3.6 to my own design with the same
microcontroller.</p>

<h3 id="mk66f-keyboard-controller">MK66F keyboard controller</h3>

<p><img src="/Bilder/kinx-mk66f.jpg" width="100%"></p>

<p>To make development pleasant, I connected a USB-to-serial adapter (to UART0) and
a “rebootor” (to PROGHEAD): another Teensy with a special firmware to trigger
programming mode. This way, I could set my editor’s <code>compile-command</code> to <code>make
&amp;&amp; teensy_loader_cli -r …</code>, compiling the code, uploading and booting into the
resulting firmware with a single keyboard shortcut.</p>

<p>I based the firmware for this controller on NXP’s SDK examples, to ensure I get
a well-tested and maintained USB stack for the USBHS port. I did some
measurements to confirm the stack does not add measurable extra latency, so I
did not see any value in me maintaining a custom USB stack.</p>

<p>The firmware can be found at <a href="https://github.com/kinx-project/mk66f-fw">https://github.com/kinx-project/mk66f-fw</a></p>

<p>The hardware can be found at <a href="https://github.com/kinx-project/mk66f-hw">https://github.com/kinx-project/mk66f-hw</a></p>

<p>Using USB 2.0 High Speed leaves us at an input latency within [0ms, 0.225ms]:</p>

<ul>
<li>≤ 0.1ms scan latency</li>
<li>0ms debounce latency</li>
<li>≤ 0.125ms USB poll latency</li>
</ul>

<h3 id="lessons-learnt">Lessons learnt</h3>

<ul>
<li><p>In the future, I will base custom designs on the vendor’s development board
(instead of on the Teensy). This way, the vendor-provided code could be used
without any modifications.</p></li>

<li><p>While the Teensy bootloader means getting started with the microcontroller
just requires a USB port, using a JTAG connector for development would be more
powerful: not only does it replace the combination of Teensy bootloader,
serial and rebootor, but it also supports debugging with gdb.</p></li>
</ul>

<h3 id="next-up">Next up</h3>

<p>The <a href="/posts/2018-04-17-kinx-usb-hub/">second post motivates and describes building a drop-in replacement USB
hub</a> for the Kinesis Advantage keyboard.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[kinX: latency measurement]]></title>
    <link href="https://michael.stapelberg.de/posts/2018-04-17-kinx-latency-measurement/"/>
    <id>https://michael.stapelberg.de/posts/2018-04-17-kinx-latency-measurement/</id>
    <published>2018-04-17T17:49:00+02:00</published>
    <updated>2018-04-17T17:49:00+02:00</updated>
    <content type="html"><![CDATA[

<p>This post is part of a <a href="/posts/2018-04-17-kinx">series of posts about the kinX project</a>.</p>

<h3 id="latency-measurement">Latency measurement</h3>

<p>End-to-end latency consists of 3 parts:</p>

<ol>
<li>input latency (keyboard)</li>
<li>processing latency (computer)</li>
<li>output latency (monitor)</li>
</ol>

<p>During the development of the kinX keyboard controller, I realized that
measuring processing latency was quite simple with my hardware: I could start a
timer when sending a key press HID report to the computer and measure the
elapsed time when I would receive a reply from the computer.</p>

<p>The key to send is the Caps Lock key, because unlike other keys it results in a
reply: a HID report telling the keyboard to turn the Caps Lock LED on.</p>

<p><img src="/Bilder/kinx-latency-measurement-device.svg"></p>

<h3 id="measurement-device">Measurement device</h3>

<p>To make this measurement technique accessible to as many people as possible, I
decided to pull it out of my kinX keyboard controller and instead build it using
the FRDM-K66F evaluation board, which uses the same microcontroller.</p>

<p>The FRDM-K66F can be bought for about 60 USD at big electronics shops, e.g. Digi-Key.</p>

<p>Find the firmware at <a href="https://github.com/kinx-project/measure-fw">https://github.com/kinx-project/measure-fw</a></p>

<h3 id="baseline">Baseline</h3>

<p>To determine the lowest processing latency one can possibly get for userspace
applications on a Linux system, I wrote a small program which uses Linux’s evdev
API to receive key presses and react to a Caps Lock keypress as quickly as it
can by turning the Caps Lock LED on.</p>

<p>Find the program at <a href="https://github.com/kinx-project/measure-evdev">https://github.com/kinx-project/measure-evdev</a></p>

<p>The following layers are exercised when measuring this program:</p>

<ul>
<li>USB host controller</li>
<li>Linux kernel (USB and input subsystems)</li>
<li>input event API (evdev)</li>
</ul>

<p>Notably, graphical interfaces such as X11 or Wayland are excluded.</p>

<p>The measurements can be verified using Wireshark’s usbmon capturing, which
provides a view of the USB bus from the computer’s perspective, excluding USB
poll latency and USB transaction time.</p>

<p>Using Ubuntu 17.10, I measured a processing latency of 152 μs on average.</p>

<h3 id="emacs">Emacs</h3>

<p>Now let’s see whether my current editor of choice adds significant latency.</p>

<p>Using a few lines of Emacs Lisp, I instructed Emacs to turn on the Caps Lock LED
whenever a key is inserted into the current buffer. In combination with
remapping the Caps Lock key to any other key (e.g. “a”), this allows us to
measure Emacs’s processing latency.</p>

<p>On the same Ubuntu 17.10 installation used above, Emacs 25.2.2 adds on average
278 μs to the baseline processing latency.</p>

<p>Find the code at <a href="https://github.com/kinx-project/measure-emacs">https://github.com/kinx-project/measure-emacs</a></p>

<h3 id="end-to-end-latency">End-to-end latency</h3>

<p>With the kinX keyboard controller, we can achieve the following end-to-end latency:</p>

<table>
<thead>
<tr>
<th>contributor</th>
<th>latency</th>
</tr>
</thead>

<tbody>
<tr>
<td>Matrix scan</td>
<td>≈ 100 μs</td>
</tr>

<tr>
<td>USB poll</td>
<td>≈ 125 μs</td>
</tr>

<tr>
<td>Linux</td>
<td>≈ 152 μs</td>
</tr>

<tr>
<td>Emacs</td>
<td>≈ 278 μs</td>
</tr>
</tbody>
</table>

<p><br>This sums up to ≈ 655 μs on average. On top of that, we have output latency
within [0, 16ms] due to the 60 Hz refresh rate of our monitors.</p>

<p>Note that using a compositor adds one frame of output latency.</p>

<h3 id="input-latency-perception">Input latency perception</h3>

<p>A natural question to ask is how well humans can perceive input latency. After
all, keyboards have been manufactured for many years, and if input latency was
really that important, surely manufacturers would have picked up on this fact by
now?</p>

<p>I ran a little unscientific experiment in the hope to further my understanding
of this question at the most recent Chaos Communication Congress in Leipzig.</p>

<p>In the experiment, I let 17 people play a game on a specially prepared
keyboard. In each round, the game reconfigures the keyboard to either have
additional input latency or not, decided at random. The player can then type a
few keys and make a decision. If the player can correctly indicate whether
additional input latency was present in more than 50% of the cases, the player
is said to be able to distinguish latency at that level. On each level, the game
decreases the additional input latency: it starts with 100ms, then 75ms, then
50ms, etc.</p>

<p>The most sensitive player reliably recognized an additional 15ms of input
latency.</p>

<p>Some players could not distinguish 75ms of additional input latency.</p>

<p>Every player could distinguish 100ms of additional input latency.</p>

<p>My take-away is that many people cannot perceive slight differences in input
latency at all, explaining why keyboard manufacturers don’t optimize for low
latency.</p>

<p>Reducing input latency still seems worthwhile to me: even if the reduction
happens under the threshold at which you can perceive differences in input
latency, it buys you more leeway in the entire stack. In other words, you might
now be able to turn on an expensive editor feature which previously slowed down
typing too much.</p>

<h3 id="conclusion">Conclusion</h3>

<p>When I started looking into input latency, my keyboard had dozens of
milliseconds of latency. I found an easy win in the old firmware, then hit a
wall, started the kinX project and eventually ended up with a keyboard with just
0.225ms input latency.</p>

<p>Even if I had not reduced the input latency of my keyboard at all, I feel that
this project was a valuable learning experience: I now know a lot more about PCB
design, ARM microcontrollers, USB, HID, etc.</p>

<p>Typing on a self-built keyboard feels good: be it because of the warm fuzzy
feeling of enjoying the fruits of your labor, or whether the input latency
indeed is lower, I’m happy with the result either way.</p>

<p>Lastly, I can now decidedly claim that the processing latency of modern
computers is perfectly fine (remember our 152 μs + 278 μs measurement for
Linux + Emacs), and as long as you pick decent peripherals, your end-to-end
latency will be fine, too.</p>

<h3 id="what-s-next">What’s next?</h3>

<p>By far the biggest factor in the end-to-end latency is the monitor’s refresh
rate, so getting a monitor with a high refresh rate and no additional processing
latency would be the next step in reducing the end-to-end latency.</p>

<p>As far as the keyboard goes, the matrix scan could be eliminated by wiring up
each individual key to a microcontroller with enough GPIO pins. The USB poll
delay could be eliminated by switching to USB 3, but I don’t know of any
microcontrollers which have USB 3 built-in yet. Both of these improvements are
likely not worth the effort.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[kinX: overview]]></title>
    <link href="https://michael.stapelberg.de/posts/2018-04-17-kinx/"/>
    <id>https://michael.stapelberg.de/posts/2018-04-17-kinx/</id>
    <published>2018-04-17T17:49:00+02:00</published>
    <updated>2018-04-17T17:49:00+02:00</updated>
    <content type="html"><![CDATA[<p>The kinX project is described in a series of blog posts:</p>

<ul>
<li><p>While not strictly a part of this series, <a href="/posts/2013-03-21-kinesis_custom_controller/">“Hacking your own Kinesis keyboard
controller”</a> describes the first
controller I built in 2013 (maybe interesting for context).</p></li>

<li><p>The <a href="/posts/2018-04-17-kinx-keyboard-controller/">first post introduces the kinX, a keyboard
controller</a> with merely 0.2ms of
input latency (including USB).</p></li>

<li><p>The <a href="/posts/2018-04-17-kinx-usb-hub/">second post motivates and describes building a drop-in replacement USB
hub</a> for the Kinesis Advantage keyboard.</p></li>

<li><p>The <a href="/posts/2018-04-17-kinx-latency-measurement/">last post introduces the processing latency measurement firmware for the
FRDM-K66F development board</a> and
draws a conclusion.</p></li>
</ul>

<p>You can find the project’s artifacts at <a href="https://github.com/kinx-project">https://github.com/kinx-project</a>.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[cpu(1) with Linux]]></title>
    <link href="https://michael.stapelberg.de/posts/2018-03-13-cpu/"/>
    <id>https://michael.stapelberg.de/posts/2018-03-13-cpu/</id>
    <published>2018-03-13T09:35:00+01:00</published>
    <updated>2018-03-13T09:35:00+01:00</updated>
    <content type="html"><![CDATA[

<h3 id="motivation">motivation</h3>

<p>To run the tests of my i3 Go package, I use the following command:</p>

<pre><code>go test -v go.i3wm.org/...
</code></pre>

<p>To run the tests of my i3 Go package on a different architecture, the only thing
I should need to change is to declare the architecture by setting
<code>GOARCH=arm64</code>:</p>

<pre><code>GOARCH=arm64 go test -v go.i3wm.org/...
</code></pre>

<p>“Easy!”, I hear you exclaim: “Just <code>apt install qemu</code>, and you can transparently
emulate architectures”. But what if I want to run my tests on a native machine,
such as the various <a href="https://db.debian.org/machines.cgi?sortby=purpose&amp;sortorder=dsc">Debian porter
boxes</a>? Down
the rabbit hole we go…</p>

<h3 id="cpu-1">cpu(1)</h3>

<p>On Plan 9, the <a href="http://man.cat-v.org/plan_9/1/cpu">cpu(1)</a> command allows
transparently using the CPU of dedicated compute servers. This has fascinated me
for a long time, so I tried to replicate the functionality in Linux.</p>

<h3 id="reverse-sshfs">reverse sshfs</h3>

<p>One of the key insights this project is built on is that
<a href="https://manpages.debian.org/stretch/sshfs/sshfs.1"><code>sshfs(1)</code></a> can be used over
an already-authenticated channel, so you don’t need to do awkward reverse
port-forwardings or even allow the remote machine SSH access to your local
machine.</p>

<p>I learnt this trick from the 2014 <a href="https://blog.dhampir.no/content/reverse-sshfs-mounts-fs-push">boltblog post “Reverse SSHFS mounts (fs
push)”</a>.</p>

<p>The post uses <a href="https://manpages.debian.org/stretch/vde2/dpipe.1"><code>dpipe(1)</code></a>’s
bidirectional wiring of stdin/stdout (as opposed to a unidirectional wiring like
in UNIX pipes).</p>

<p>Instead of clumsily running <code>dpipe</code> in a separate window, I encapsulated the
necessary steps in a little Go program I call <code>cpu</code>. The reverse sshfs principle
looks like this in Go:</p>

<pre><code>sftp := exec.Command(&quot;/usr/lib/openssh/sftp-server&quot;)
stdin, _ := sftp.StdinPipe()
stdout, _ := sftp.StdoutPipe()
session.Stdin = stdout
session.Stdout = stdin
sftp.Stderr = os.Stderr
session.Stderr = os.Stderr
const (
	host = &quot;&quot;
	src  = &quot;/&quot;
	mnt  = &quot;/mnt&quot;
)
session.Start(fmt.Sprintf(&quot;sshfs %s:%s %s -o slave&quot;, host, src, mnt))
sftp.Start()
</code></pre>

<p>Here’s how the tool looks in action:</p>

<script src="https://asciinema.org/a/Q1BWLcdtIMOE5SCHOzu1eqcOE.js" id="asciicast-Q1BWLcdtIMOE5SCHOzu1eqcOE" async></script>

<h3 id="binfmt-misc">binfmt_misc</h3>

<p>Now that we have a tool which will make our local file system available on the
remote machine, let’s integrate it into our <code>go test</code> invocation.</p>

<p>While we don’t want to modify the <code>go</code> tool, we can easily teach our kernel how
to run aarch64 ELF binaries using
<a href="https://www.kernel.org/doc/html/v4.14/admin-guide/binfmt-misc.html">binfmt_misc</a>.</p>

<p>I modified the existing <code>/var/lib/binfmts/qemu-aarch64</code>’s interpreter field to
point to <code>/home/michael/go/bin/porterbox-aarch64</code>, followed by <code>update-binfmts
--enable qemu-aarch64</code> to have the kernel pick up the changes.</p>

<p><code>porterbox-aarch64</code> is a wrapper invoking <code>cpu</code> like so:</p>

<pre><code>cpu \
  -host=rpi3 \
  unshare \
    --user \
    --map-root-user \
    --mount-proc \
    --pid \
    --fork \
    /bindmount.sh \
      \$PWD \
      $PWD \
      $@
</code></pre>

<p>Because it’s subtle:</p>

<ul>
<li><code>\$PWD</code> refers to the directory in which the reverse sshfs was mounted by <code>cpu</code>.</li>
<li><code>$PWD</code> refers to the working directory in which <code>porterbox-aarch64</code> was called.</li>
<li><code>$@</code> refers to the original command with which <code>porterbox-aarch64</code> was called.</li>
</ul>

<h3 id="bindmount">bindmount</h3>

<p>bindmount is a small shell script preparing the bind mounts:</p>

<pre><code>#!/bin/sh

set -e

remote=&quot;$1&quot;
shift
wd=&quot;$1&quot;
shift

# Ensure the executable (usually within /tmp) is available:
exedir=$(dirname &quot;$1&quot;)
mkdir -p &quot;$exedir&quot;
mount --rbind &quot;$remote$exedir&quot; &quot;$exedir&quot;

# Ensure /home is available:
mount --rbind &quot;$remote/home&quot; /home

cd &quot;$wd&quot;
&quot;$@&quot;
</code></pre>

<h3 id="demo">demo</h3>

<p>This is what all of the above looks like in action:</p>

<script src="https://asciinema.org/a/Mjb66iHIbBfGuK5lEMnLt0UzS.js" id="asciicast-Mjb66iHIbBfGuK5lEMnLt0UzS" async></script>

<h3 id="layers">layers</h3>

<p>Putting all of the above puzzle pieces together, we end up with the following
picture:</p>

<pre><code>go test
├ compile test program for GOARCH=arm64
└ exec test program (on host)
  └ binfmt_misc
    └ run porterbox-aarch64
      └ cpu -host=rpi3
        ├ reverse sshfs
        └ bindmount.sh
          └ unshare --user
            ├ bind /home, /tmp
            └ run test program (on target)
</code></pre>

<h3 id="requirements">requirements</h3>

<p>On the remote host, the following requirements need to be fulfilled:</p>

<ul>
<li><code>apt install sshfs</code>, which also activates the FUSE kernel module</li>
<li><code>sysctl -w kernel.unprivileged_userns_clone=1</code></li>
</ul>

<p>If the tests require any additional dependencies (the tests in question require
<code>Xvfb</code> and <code>i3</code>), those need to be installed as well.</p>

<p>On Debian porter boxes, you can install the dependencies in an <a href="https://dsa.debian.org/doc/schroot/"><code>schroot</code>
session</a>. Note that I wasn’t able to test
this yet, as porter boxes lacked all requirements at the time of writing.</p>

<p>Unfortunately, <a href="https://wiki.debian.org/Multiarch">Debian’s Multi-Arch</a> does not
yet include binaries. Otherwise, one might use it to help out with the
dependencies: one could overlay the local <code>/usr/bin/aarch64-linux-gnu/</code> on the
remote <code>/usr/bin</code>.</p>

<h3 id="conclusion">conclusion</h3>

<p>On first glance, this approach works as expected. Time will tell whether it’s
useful in practice or just an interesting one-off exploration.</p>

<p>From a design perspective, there are a few open questions:</p>

<ul>
<li>Making available only <code>/home</code> might not be sufficient. But making available
<code>/</code> doesn’t work because <code>sshfs</code> does not support device nodes such as
<code>/dev/null</code>.</li>
<li>Is there a way to implement this without unprivileged user namespaces (which
are disabled by default on Linux)? Essentially, I think I’m asking for <a href="https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs#Union_directories_and_namespaces">Plan
9’s union directories and
namespaces</a>.</li>
<li>In similar spirit, can binfmt_misc be used per-process?</li>
</ul>

<p>Regardless, if this setup stands the test of time, I’ll polish and publish the
tools.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Off-site backups with an apu2c4]]></title>
    <link href="https://michael.stapelberg.de/posts/2018-01-13-offsite-backup-apu2c4/"/>
    <id>https://michael.stapelberg.de/posts/2018-01-13-offsite-backup-apu2c4/</id>
    <published>2018-01-13T17:30:00+01:00</published>
    <updated>2018-01-13T17:30:00+01:00</updated>
    <content type="html"><![CDATA[

<h3 id="background">Background</h3>

<p>A short summary of my backup strategy is: I run daily backups to my
<a href="/Artikel/gigabit-nas-coreos">NAS</a>. In order to recover from risks like my
apartment burning down or my belongings being stolen, I like to keep one copy of
my data off-site, updated less frequently.</p>

<p>I used to store off-site backups with the “unlimited storage” offerings of
various cloud providers.</p>

<p>These offers follow a similar pattern: they are announced, people sign up and
use a large amount of storage, the provider realizes they cannot make enough
money off of this pricing model, and finally the offer is cancelled.</p>

<p>I went through this two times, and my friend Mark’s <a href="https://bryars.eu/2017/10/backup-pi/">similar experience and
home-grown solution</a> inspired me to also
build my own off-site backup.</p>

<h3 id="introduction">Introduction</h3>

<p>I figured the office would make a good place for an external hard disk: I’m
there every workday, it’s not too far away, and there is good internet
connectivity for updating the off-site backup.</p>

<p>Initially, I thought just leaving the external hard disk there and updating it
over night by bringing my laptop to the office every couple of weeks would be
sufficient.</p>

<p>Now I know that strategy doesn’t work for me: the time would never be good
(“maybe I’ll unexpectedly need my laptop tonight!”), I would forget, or I would
not be in the mood.</p>

<p>Lesson learnt: <strong>backups must not require continuous human involvement</strong>.</p>

<p>The rest of this article covers the hardware I decided to use and the software
setup.</p>

<h3 id="hardware">Hardware</h3>

<p>The external hard disk enclosure is a <a href="https://www.alternate.de/Sharkoon/Swift-Case-PRO-USB-3-0-Laufwerksgeh%C3%A4use/html/product/1148212">T3US41 Sharkoon Swift Case PRO USB
3.0</a>
for 25 €.</p>

<p>The enclosed disk is a HGST 8TB drive for which I paid 290 € in mid 2017.</p>

<p>For <a href="/Artikel/rgb2r-network">providing internet at our yearly retro computing
event</a>, I still had a <a href="https://pcengines.ch/apu2c4.htm">PC Engines
apu2c4</a> lying around, which I repurposed for my
off-site backups. For this year’s retro computing event, I’ll either borrow it
(setting it up is quick) or buy another one.</p>

<p>The apu2c4 has two USB 3.0 ports, so I can connect my external hard disk to it
without USB being a bottle-neck.</p>

<h3 id="setup-installation">Setup: installation</h3>

<p>On the apu2c4, I installed Debian “stretch” 9, the latest Debian stable version
at the time of writing. I prepared a USB thumb drive with the netinst image:</p>

<pre><code>% wget https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/debian-9.2.1-amd64-netinst.iso
% cp debian-9.2.1-amd64-netinst.iso /dev/sdb
</code></pre>

<p>Then, I…</p>

<ul>
<li>plugged the USB thumb drive into the apu2c4</li>
<li>On the serial console, pressed F10 (boot menu), then 1 (boot from USB)</li>
<li>In the Debian installer, selected Help, pressed F6 (special boot parameters), entered <code>install console=ttyS0,115200n8</code></li>
<li>installed Debian as usual.</li>
<li>Manually ran <code>update-grub</code>, so that GRUB refers to the boot disk by UUID instead of <code>root=/dev/sda1</code>. Especially once the external hard disk is connected, device nodes are unstable.</li>
<li>On the serial console, pressed F10 (boot menu), then 4 (setup), then c to move the mSATA SSD to number 1 in boot order</li>
<li>Connected the external hard disk</li>
</ul>

<h3 id="setup-persistent-reverse-ssh-tunnel">Setup: persistent reverse SSH tunnel</h3>

<p>I’m connecting the apu2c4 to a guest network port in our office, to keep it
completely separate from our corporate infrastructure. Since we don’t have
permanently assigned publically reachable IP addresses on that guest network, I
needed to set up a reverse tunnel.</p>

<p>First, I created an SSH private/public keypair using <a href="https://manpages.debian.org/stretch/openssh-client/ssh-keygen.1"><code>ssh-keygen(1)</code></a>.</p>

<p>Then, I created a user account for the apu2c4 on my NAS (using cloud-config),
where the tunnel will be terminated. This account’s SSH usage is restricted to
port forwardings only:</p>

<pre><code>users:
  - name: apu2c4
    system: true
    ssh-authorized-keys:
      - &quot;restrict,command=\&quot;/bin/false\&quot;,port-forwarding ssh-rsa AAAA…== root@stapelberg-apu2c4&quot;
</code></pre>

<p>On the apu2c4, I installed the <code>autossh</code> Debian package (see the
<a href="https://manpages.debian.org/stretch/autossh/autossh.1"><code>autossh(1)</code></a> manpage
for details) and created the systemd unit file
<code>/etc/systemd/system/autossh-nas.service</code> with the following content:</p>

<pre><code>[Unit]
Description=autossh reverse tunnel
After=network.target
Wants=network-online.target

[Service]
Restart=always
StartLimitIntervalSec=0
Environment=AUTOSSH_GATETIME=0
ExecStart=/usr/bin/autossh -M 0 -N -o &quot;ServerAliveInterval 60&quot; -o &quot;ServerAliveCountMax 3&quot; -o &quot;ExitOnForwardFailure yes&quot; apu2c4@nas.example.net -R 2200:localhost:22

[Install]
WantedBy=multi-user.target
</code></pre>

<p>After enabling and starting the unit using <code>systemctl enable --now autossh-nas</code>,
the apu2c4 connected to the NAS and set up a reverse port-forwarding.</p>

<p>On the NAS, I configure SSH like so in my <code>/root/.ssh/config</code>:</p>

<pre><code>Host apu2c4
  Hostname localhost
  Port 2200
  User root
  IdentitiesOnly yes
</code></pre>

<p>Finally, I authorized the public key of my NAS to connect to the apu2c4.</p>

<p>Note that this concludes the setup of the apu2c4: the device’s only purpose is
to make the external hard disk drive available remotely to my NAS, clean and
simple.</p>

<h3 id="setup-full-disk-encryption">Setup: full-disk encryption</h3>

<p>I decided to not store the encryption key for the external hard disk on the
apu2c4, to have piece of mind in case the hard disk gets misplaced or even
stolen. Of course I trust my co-workers, but this is a matter of principle.</p>

<p>Hence, I amended my NAS’s cloud-config setup like so (of course with a stronger
key):</p>

<pre><code>write_files:
  - path: /root/apu2c4.lukskey
    permissions: 0600
    owner: root:root
    content: |
    ABCDEFGHIJKL0123456789
</code></pre>

<p>…and configured the second key slot of the external hard disk to use this key.</p>

<h3 id="setup-backup-script">Setup: Backup script</h3>

<p>I’m using a script roughly like the following to do the actual backups:</p>

<pre><code>#!/bin/bash
# vi:ts=4:sw=4:et
set -e

/bin/ssh apu2c4 cryptsetup luksOpen --key-file - /dev/disk/by-id/ata-HGST_HDN1234 offline_crypt &lt; /root/apu2c4.lukskey

/bin/ssh apu2c4 mount /dev/mapper/offline_crypt /mnt/offsite

# step 1: update everything but /backups
echo &quot;$(date +'%c') syncing NAS data&quot;

(cd /srv &amp;&amp; /usr/bin/rsync --filter 'exclude /backup' -e ssh -ax --relative --numeric-ids ./ apu2c4:/mnt/offsite)

# step 2: copy the latest backup
hosts=$(ls /srv/backup/)
for host in $hosts
do
  latestremote=$(ls /srv/backup/${host}/ | tail -1)
  latestlocal=$(/bin/ssh apu2c4 ls /mnt/offsite/backup/${host} | tail -1)
  if [ &quot;$latestlocal&quot; != &quot;$latestremote&quot; ]
  then
    echo &quot;$(date +'%c') syncing $host (offline: ${latestlocal}, NAS: ${latestremote})&quot;
    /bin/ssh apu2c4 mkdir -p /mnt/offsite/backup/${host}
    (cd /srv &amp;&amp; /usr/bin/rsync -e ssh -ax --numeric-ids ./backup/${host}/${latestremote}/ apu2c4:/mnt/offsite/backup/${host}/${latestremote} --link-dest=../${latestlocal})

    # step 3: delete all previous backups
    echo &quot;$(date +'%c') deleting everything but ${latestremote} for host ${host}&quot;
    ssh apu2c4 &quot;find /mnt/offsite/backup/${host} \! \( -path \&quot;/mnt/offsite/backup/${host}/${latestremote}/*\&quot; -or -path \&quot;/mnt/offsite/backup/${host}/${latestremote}\&quot; -or -path \&quot;/mnt/offsite/backup/${host}\&quot; \) -delete&quot;
  fi
done

/bin/ssh apu2c4 umount /mnt/offsite
/bin/ssh apu2c4 cryptsetup luksClose offline_crypt
/bin/ssh apu2c4 hdparm -Y /dev/disk/by-id/ata-HGST_HDN1234
</code></pre>

<p>Note that this script is not idempotent, lacking in error handling and won’t be
updated. It merely serves as an illustration of how things could work, but
specifics depend on your backup.</p>

<p>To run this script weekly, I created the following cloud-config on my NAS:</p>

<pre><code>coreos:
  units:
    - name: sync-offsite.timer
      command: start
      content: |
        [Unit]
        Description=sync backups to off-site storage

        [Timer]
        OnCalendar=Sat 03:00

    - name: sync-offsite.service
      content: |
        [Unit]
        Description=sync backups to off-site storage
        After=docker.service srv.mount
        Requires=docker.service srv.mount

        [Service]
        Type=oneshot

        ExecStart=/root/sync-offsite-backup.sh
</code></pre>

<h3 id="improvement-bandwidth-throttling">Improvement: bandwidth throttling</h3>

<p>In case your office (or off-site place) doesn’t have a lot of bandwidth
available, consider throttling your backups. Thus far, I haven’t had the need.</p>

<h3 id="improvement-rtc-based-wake-up">Improvement: RTC-based wake-up</h3>

<p>I couldn’t figure out whether the apu2c4 supports waking up based on a real-time
clock (RTC), and if yes, whether that works across power outages.</p>

<p>If so, one could keep it shut down (or suspended) during the week, and only
power it up for the actual backup update. The downside of course is that
any access (such as for restoring remotely) require physical presence.</p>

<p>If you know the answer, please send me an email.</p>

<h3 id="conclusion">Conclusion</h3>

<p>The presented solution is easier to integrate than most cloud storage
solutions.</p>

<p>Of course my setup is less failure-tolerant than decent cloud storage providers,
but given the low probability of a catastrophic event (e.g. apartment burning
down), it’s fine to just order a new hard disk or apu2c4 when either of the two
fails — for this specific class of backups, that’s an okay trade-off to make.</p>

<p>The upside of my setup is that the running costs are very low: the apu2c4’s few
watts of electricity usage are lost in the noise, and syncing a few hundred MB
every week is cheap enough these days.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Dell 8K4K monitor (Dell UP3218K)]]></title>
    <link href="https://michael.stapelberg.de/posts/2017-12-11-dell-up3218k/"/>
    <id>https://michael.stapelberg.de/posts/2017-12-11-dell-up3218k/</id>
    <published>2017-12-11T10:05:00+01:00</published>
    <updated>2017-12-11T10:05:00+01:00</updated>
    <content type="html"><![CDATA[

<h3 id="background">Background</h3>

<p>Ever since I first used a MacBook Pro with Retina display back in 2013, I’ve
been madly in love with hi-DPI displays. I had seen the device before, and
marvelled at brilliant font quality with which scientific papers would be
rendered. But it wasn’t until I had a chance to use the device for a few hours
to make i3 compatible with hi-DPI displays that I realized what a difference it
makes in the day-to-day life.</p>

<p>Note that when I say “hi-DPI display”, I mean displays with an integer multiple
of 96 dpi, for example displays with 192 dpi or 288 dpi. I explain this because
some people use the same term to mean “anything more than 96 dpi”.</p>

<p>In other words, some people are looking for many pixels (e.g. running a 32 inch
display with 3840x2160 pixels, i.e. 137 dpi, with 100% scaling), whereas I
desire crisp/sharp text (i.e. 200% scaling).</p>

<p>Hence, in 2014, I bought the Dell UP2414Q with 3840x2160 on 24&quot; (185 dpi), which
was one of the first non-Apple devices to offer a dpi that Apple would market as
“Retina”.</p>

<p>After getting the Dell UP2414Q, I replaced all displays in my life with hi-DPI
displays one by one. I upgraded my phone, my personal laptop, my work laptop and
my monitor at work.</p>

<h3 id="dell-up3218k">Dell UP3218K</h3>

<p>In January 2017, Dell introduced the Dell Ultrasharp UP3218K monitor at the
Consumer Electronics Show (CES). It is the world’s first available 8K monitor,
meaning it has a resolution of 7680x4320 pixels at a refresh rate of 60 Hz. The
display’s dimensions are 698.1mm by 392.7mm (80cm diagonal, or 31.5 inches),
meaning the display shows 280 dpi.</p>

<p>While the display was available in the US for quite some time, it took until
October 2017 until it became available in Switzerland.</p>

<h3 id="compatibility">Compatibility</h3>

<p>The UP3218K requires connecting two separate DisplayPort 1.4 cables in order to
reach the native resolution and refresh rate. When connecting only one cable,
you will be limited to a refresh rate of 30 Hz, which is a very noticeable
annoyance on any display: you can literally see your mouse cursor lag
behind. Ugh.</p>

<p>Note that this mode of connection does not use Multi-Stream Transport (MST),
which was a trick that first-generation 4K displays used. Instead, it uses the
regular Single-Stream Transport (SST), but two cables.</p>

<p>As of November 2017, only latest-generation graphics cards support DisplayPort
1.4 at all, with e.g. the <a href="https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1060/">nVidia GTX
1060</a>
being marketed as “DisplayPort 1.2 Certified, DisplayPort 1.3/1.4 Ready”.</p>

<h4 id="amd-radeon-pro-wx7100">AMD Radeon Pro WX7100</h4>

<p>Hence, I thought I would play it safe and buy a graphics card which is
explicitly described as compatible with the UP3218K: I ordered an AMD Radeon Pro
WX7100.</p>

<p>Unfortunately, I have to report that the WX7100 is only able to drive the
monitor at native resolution when using Windows. On Linux, I was limited to
1920x1080 at 60Hz (!) when using the Open Source amdgpu driver. With the Closed
Source amdgpu-pro driver, I reached 3840x2160 at 60Hz, which is still not the
native resolution. Also, the amdgpu-pro driver is a hassle to install: it
requires an older kernel and isn’t packaged well in Debian.</p>

<h4 id="nvidia-geforce-gtx-1060">nVidia GeForce GTX 1060</h4>

<p>I returned the WX7100 in exchange for the cheapest and most quiet GeForce 10
series card with 2 DisplayPort outputs I could find. My choice was the <a href="https://www.msi.com/Graphics-card/GeForce-GTX-1060-GAMING-X-6G.html">MSI
GeForce GTX 1060 GAMING X 6G (MSI
V328-001R)</a>. The
card seems like overkill, given that I don’t intend to play games on this
machine, but lower-end cards all come with at most one DisplayPort output.</p>

<p>Regardless, I am happy with the card. It indeed is silent, and with the Closed
Source driver, it powers the UP3218K without any trouble. Notably, it supports
RandR 1.5, which I’ll talk about a bit more later.</p>

<h4 id="compatibility-matrix">Compatibility Matrix</h4>

<table>
<thead>
<tr>
<th>Operating System</th>
<th>Graphics Card</th>
<th align="center">Driver</th>
<th>Resolution</th>
</tr>
</thead>

<tbody>
<tr>
<td>Windows</td>
<td>Radeon WX7100</td>
<td align="center">yes</td>
<td>7680x4320 @ 60 Hz</td>
</tr>

<tr>
<td>Windows</td>
<td>GeForce 1060</td>
<td align="center">yes</td>
<td>7680x4320 @ 60 Hz</td>
</tr>

<tr>
<td>Linux</td>
<td>Radeon WX7100</td>
<td align="center">amdgpu</td>
<td>1920x1080 @ 60 Hz</td>
</tr>

<tr>
<td>Linux</td>
<td>Radeon WX7100</td>
<td align="center">pro</td>
<td>3840x2160 @ 60 Hz</td>
</tr>

<tr>
<td>Linux</td>
<td>GeForce 1060</td>
<td align="center">nVidia</td>
<td>7680x4320 @ 60 Hz</td>
</tr>
</tbody>
</table>

<h4 id="recommendation">Recommendation</h4>

<p>If you want to play it safe, buy an nVidia card of the GeForce 10 series. Verify
that it says “DisplayPort 1.4 Ready”, and that it comes with two DisplayPort
outputs.</p>

<p>I read about <a href="https://heise.de/-3900646">improvements of the amdgpu driver for the upcoming Linux
4.15</a>, but I don’t know whether that will help with
the problems at hand.</p>

<h3 id="impressions">Impressions</h3>

<p>The unboxing experience is well-designed, and all components make a good
impression. All cables which you will need (two DisplayPort cables, a power
cable, a USB cable) are included and seem to be of high quality.</p>

<p>The display has a thin bezel, much thinner than my other monitors ViewSonic
VX2475Smhl-4K or Dell UP2414Q.</p>

<p>The power LED is white and not too bright. The on-screen menu reacts quickly and
is reasonably intuitive.</p>

<p>The built-in USB hub works flawlessy, even with devices which don’t work on my
standalone USB3 hub (for reasons which I have yet to find out).</p>

<h4 id="display-quality">Display Quality</h4>

<p>The display quality of the screen is stunningly good.</p>

<p>It was only when I configured 300% scaling that I realized why some Chromebooks
had a distinctly different look and feel from other computers I had used: I
always assumed they differed in font rendering somehow, but the actual
difference is just the screen DPI: fonts look distinctly better with 288 dpi
than with 192 dpi, which of course looks better than 96 dpi.</p>

<p>Some people might wonder whether an 8K display is any better than a 4K display,
and I now can answer that question with a decisive “yes, one can easily see the
difference”. I’m not sure if the difference between a 288 dpi and a 384 dpi
display would be visible, but we’ll see when we get there :-).</p>

<h4 id="glossy">Glossy</h4>

<p>What I didn’t expect is that the UP3218K is a glossy display, as opposed to a
matte display. Depending on the brightness and colors, you might see
reflections. With my preferred brightness of 50%, I can clearly see reflections
when displaying darker colors, e.g. on a black terminal emulator background, or
even in my grey Emacs theme.</p>

<p>While one can mentally ignore the reflections after a little while, I still
consider the glossyness a mild annoyance. I hope as 8K displays become more
prevalent, display vendors will offer matte 8K displays as well.</p>

<h4 id="scaling">Scaling</h4>

<p>I found it interesting that the display works well in both 200% scaling and 300%
scaling.</p>

<p>When running the display at 200% scaling, you get 3840x2160 (4K resolution)
“logical pixels”, but sharper.</p>

<p>When running the display at 300% scaling, you get 2560x1440 “logical pixels”,
but extremely sharp.</p>

<p>I would say it is a subjective preference which of the two settings to use. Most
likely, people who prefer many pixels would run the display at 200%, whereas I
prefer the 300% scaling mode for the time being.</p>

<h3 id="linux-compatibility-configuration">Linux compatibility / configuration</h3>

<p>To use this display without gross hacks, ensure all relevant components in your
software stack support RandR 1.5. My known working configuration is:</p>

<ul>
<li>Xorg 1.19.5</li>
<li>nVidia driver 375.82</li>
<li>libxcb 1.12</li>
<li>i3 4.14</li>
<li>i3lock 2.10</li>
</ul>

<p>With the following command, you can create a RandR MONITOR object spanning the
DisplayPort outputs DP-4 and DP-2:</p>

<pre><code>xrandr --setmonitor up3218k auto DP-4,DP-2
</code></pre>

<p>I place this command in my <code>~/.xsession</code> before starting <a href="https://i3wm.org/">i3</a>.</p>

<p>Theoretically, Xorg could create a MONITOR object automatically. I filed <a href="https://bugzilla.freedesktop.org/show_bug.cgi?id=103794">a
feature request</a> for
this.</p>

<h3 id="scaling-compatibility">Scaling compatibility</h3>

<p>With regards to scaling issues, the situation is very similar to any other
monitor which requires scaling. Applications which were updated to support 200%
scaling seem to work with 300% scaling just as well.</p>

<p>Of course, applications which were not yet updated to work with scaling look
even smaller than on 200% displays, so it becomes more of a nuisance to use
them. As far as I can tell, the most likely offender are Java applications such
as JDownloader.</p>

<h3 id="buzzing-noise">Buzzing noise</h3>

<p>Unfortunately, the monitor emits a high-pitched buzzing noise, very similar to
Coil Whine. The noise is loud enough to prevent focused work without listening
to music.</p>

<p>I verified that this symptom was happening with Windows and Linux, on two
different computers, with default monitor settings, and even when no input
source was connected at all.</p>

<p>Finally, I contacted Dell support about it. In the call I received on the next
business day, they were very forthcoming and offered to replace the monitor.</p>

<p>The replacement monitor still emits some noise, but much less pronounced. I
think I can easily ignore the noise.</p>

<h3 id="wakeup-issues">Wakeup issues</h3>

<p>Rarely (less than once a week), when waking up the monitor from DPMS standby
mode, only the left half of the screen would appear on my monitor.</p>

<p>This can be fixed by turning the monitor off and on again.</p>

<p>My theory is that one of the scalers does not manage to synchronize with the
video card, but I don’t know for sure.</p>

<p>Interestingly enough, I also encountered this issue with the Dell UP2414Q I
bought in 2014. My workaround is to power down that display using its power
button in the evenings, and power it up in the mornings.</p>

<h3 id="conclusion">Conclusion</h3>

<p>For me, this monitor is worth it: I am okay with paying the hefty Research &amp;
Development tax that first-to-market products such as this monitor have. I like
to think that I’m voting with my wallet to make sure vendors notice my interest
in “Retina” displays.</p>

<p>For most people, I would recommend to wait until the second or third generation
of 8K monitors. By then, I expect most issues to be resolved, compatibility to
not be a concern, and vendors focusing on extra features. Hopefully, we’ll
eventually see matte 8K monitors with higher refresh rates than 60 Hz.</p>

<h3 id="technical-details">Technical details</h3>

<p>In the hope the following is useful (perhaps for debugging?) to anyone:</p>

<ul>
<li><a href="/dell-up3218k/edid.hex.txt">hex dump of the EDID data block</a></li>
<li><a href="/dell-up3218k/edid-decode.txt">decoded EDID block</a></li>
<li><a href="/dell-up3218k/xrandr--prop.txt"><code>xrandr --prop</code> output</a></li>
<li><a href="/dell-up3218k/xorg.0.log">Xorg.0.log</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Network setup for our retro computing event RGB2Rv17]]></title>
    <link href="https://michael.stapelberg.de/posts/2017-11-13-rgb2r-network/"/>
    <id>https://michael.stapelberg.de/posts/2017-11-13-rgb2r-network/</id>
    <published>2017-11-13T22:45:00+01:00</published>
    <updated>2017-11-13T22:45:00+01:00</updated>
    <content type="html"><![CDATA[

<p>Our computer association <a href="https://www.noname-ev.de/">NoName e.V.</a> organizes a
retro computing event called <a href="https://rgb2r.noname-ev.de/">RGB2R</a> every year,
located in Heidelberg, Germany. This year’s version is called RGB2Rv17.</p>

<p>This article describes the network setup I created for this year’s event.</p>

<p>The intention is not so much to provide a fully working setup (even though the
setup did work fine for us as-is), but rather inspire to you to create your own
network, based vaguely on what’s provided here.</p>

<h3 id="connectivity">Connectivity</h3>

<p>The venue has a DSL connection with speeds reaching 1 Mbit/s if you’re
lucky. Needless to say, that is not sufficient for the about 40 participants we
had.</p>

<p>Luckily, there is (almost) direct line of sight to my parent’s place, and my dad
recently got a 400 Mbit/s cable internet connection, which he’s happy to share
with us :-).</p>

<p><img src="/Bilder/IMG_7157.jpg" width="640" height="480" alt="WiFi antenna pole"></p>

<h3 id="hardware">Hardware</h3>

<p>For the WiFi links to my parent’s place, we used 2 <a href="http://www.tp-link.com/us/products/details/cat-37_CPE510.html">tp-link
CPE510</a> (CPE
stands for Customer Premise Equipment) on each site. The devices only have 100
Mbit/s ethernet ports, which is why we used two of them.</p>

<p>The edge router for the event venue was a <a href="https://pcengines.ch/apu2c4.htm">PC Engines
apu2c4</a>. For the Local Area Network (LAN)
within the venue, we provided a few switches and WiFi using <a href="https://www.ubnt.com/">Ubiquiti
Networks</a> access points.</p>

<h3 id="software">Software</h3>

<p>On the apu2c4, I installed Debian “stretch” 9, the latest Debian stable version
at the time of writing. I prepared a USB thumb drive with the netinst image:</p>

<pre><code>% wget https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/debian-9.2.1-amd64-netinst.iso
% cp debian-9.2.1-amd64-netinst.iso /dev/sdb
</code></pre>

<p>Then, I…</p>

<ul>
<li>plugged the USB thumb drive into the apu2c4</li>
<li>On the serial console, pressed F10 (boot menu), then 1 (boot from USB)</li>
<li>In the Debian installer, selected Help, pressed F6 (special boot parameters), entered <code>install console=ttyS0,115200n8</code></li>
<li>installed Debian as usual.</li>
</ul>

<h4 id="initial-setup">Initial setup</h4>

<p>Debian stretch comes with systemd by default, but not with
<a href="https://manpages.debian.org/stretch/systemd/systemd-networkd.8.en.html"><code>systemd-networkd(8)</code></a>
by default, so I changed that:</p>

<pre><code>edge# systemctl enable systemd-networkd
edge# systemctl disable networking
</code></pre>

<p>Also, I cleared the MOTD, placed <code>/tmp</code> on <code>tmpfs</code> and configured my usual
environment:</p>

<pre><code>edge# echo &gt; /etc/motd
edge# echo 'tmpfs /tmp tmpfs defaults 0 0' &gt;&gt; /etc/fstab
edge# wget -qO- https://d.zekjur.net | bash -s
</code></pre>

<p>I also installed a few troubleshooting tools which came in handy later:</p>

<pre><code>edge# apt install tcpdump net-tools strace
</code></pre>

<h4 id="disabling-icmp-rate-limiting-for-debugging">Disabling ICMP rate-limiting for debugging</h4>

<p>I had to learn the hard way that Linux imposes a rate-limit on outgoing ICMP
packets by default. This manifests itself as spurious timeouts in the
<code>traceroute</code> output. To ease debugging, I disabled the rate limit entirely:</p>

<pre><code>edge# cat &gt;&gt; /etc/sysctl.conf &lt;&lt;'EOT'
net.ipv4.icmp_ratelimit=0
net.ipv6.icmp.ratelimit=0
EOT
edge# sysctl -p
</code></pre>

<h4 id="renaming-network-interfaces">Renaming network interfaces</h4>

<p>Descriptive network interface names are helpful when debugging. I won’t remember
whether <code>enp0s3</code> is the interface for an uplink or the LAN, so I assigned the
names <code>uplink0</code>, <code>uplink1</code> and <code>lan0</code> to the apu2c4’s interfaces.</p>

<p>To rename network interfaces, I created a corresponding <code>.link</code> file, had the
initramfs pick it up, and rebooted:</p>

<pre><code>edge# cat &gt;/etc/systemd/network/10-uplink0.link &lt;&lt;'EOT'
[Match]
MACAddress=00:0d:b9:49:db:18

[Link]
Name=uplink0
EOT
edge# update-initramfs -u
edge# reboot
</code></pre>

<h3 id="network-topology">Network topology</h3>

<p>Because our internet provider didn’t offer IPv6, and to keep my dad out of the
loop in case any abuse issues should arise, we tunneled all of our traffic.</p>

<p>We decided to set up one tunnel per WiFi link, so that we could easily
load-balance over the two links by routing IP flows into one of the two tunnels.</p>

<p>Here’s a screenshot from the topology dashboard which I made using the
<a href="https://grafana.com/plugins/jdbranham-diagram-panel">Diagram</a> Grafana plugin:</p>

<p><img src="/Bilder/rgb2rv17-network-topology.jpg" width="943" height="536"></p>

<h3 id="network-interface-setup">Network interface setup</h3>

<p>We configured IP addresses statically on the <code>uplink0</code> and <code>uplink1</code> interface
because we needed to use static addresses in the tunnel setup anyway.</p>

<p>Note that we placed a default route in route table 110. Later on, we used
<a href="https://manpages.debian.org/stretch/iptables/iptables.8.en.html"><code>iptables(8)</code></a>
to make traffic use either of these two default routes.</p>

<pre><code>edge# cat &gt; /etc/systemd/network/uplink0.network &lt;&lt;'EOT'
[Match]
Name=uplink0

[Network]
Address=192.168.178.10/24
IPForward=ipv4

[Route]
Gateway=192.168.178.1
Table=110
EOT
</code></pre>

<pre><code>edge# cat &gt; /etc/systemd/network/uplink1.network &lt;&lt;'EOT'
[Match]
Name=uplink1

[Network]
Address=192.168.178.11/24
IPForward=ipv4

[Route]
Gateway=192.168.178.1
Table=111
EOT
</code></pre>

<h3 id="tunnel-setup">Tunnel setup</h3>

<p>Originally, I configured OpenVPN for our tunnels. However, it turned out the
apu2c4 tops out at 130 Mbit/s of traffic through OpenVPN. Notably, using two
tunnels didn’t help — I couldn’t reach more than 130 Mbit/s in total. This is
with authentication and crypto turned off.</p>

<p>This surprised me, but doesn’t seem too uncommon: on the internet, I could find
reports of similar speeds with the same hardware.</p>

<p>Given that our setup didn’t require cryptography (applications are using TLS
these days), I looked for light-weight alternatives and found Foo-over-UDP
(fou), a UDP encapsulation protocol supporting IPIP, GRE and SIT tunnels.</p>

<p>Each configured Foo-over-UDP tunnel only handles sending packets. For receiving,
you need to configure a listening port. If you want two machines to talk to each
other, you therefore need a listening port on each, and a tunnel on each.</p>

<p>Note that you need one tunnel per address family: IPIP only supports IPv4, SIT
only supports IPv6. In total, we ended up with 4 tunnels (2 WiFi uplinks with 2
address families each).</p>

<p>Also note that Foo-over-UDP provides no authentication: anyone who is able to
send packets to your configured listening port can spoof any IP address. If you
don’t restrict traffic in some way (e.g. by source IP), you are effectively
running an open proxy.</p>

<h4 id="tunnel-configuration">Tunnel configuration</h4>

<p>First, load the kernel modules and set the corresponding interfaces to UP:</p>

<pre><code>edge# modprobe fou
edge# modprobe ipip
edge# ip link set dev tunl0 up
edge# modprobe sit
edge# ip link set dev sit0 up
</code></pre>

<p>Configure the listening ports for receiving FOU packets:</p>

<pre><code>edge# ip fou add port 1704 ipproto 4
edge# ip fou add port 1706 ipproto 41

edge# ip fou add port 1714 ipproto 4
edge# ip fou add port 1716 ipproto 41
</code></pre>

<p>Configure the tunnels for sending FOU packets, using the local interface of the
<code>uplink0</code> interface:</p>

<pre><code>edge# ip link add name fou0v4 type ipip remote 203.0.113.1 local 192.168.178.10 encap fou encap-sport auto encap-dport 1704 dev uplink0
edge# ip link set dev fou0v4 up
edge# ip -4 address add 10.170.0.1/24 dev fou0v4

edge# ip link add name fou0v6 type sit remote 203.0.113.1 local 192.168.178.10 encap fou encap-sport auto encap-dport 1706 dev uplink0
edge# ip link set dev fou0v6 up
edge# ip -6 address add fd00::10:170:0:1/112 dev fou0v6 preferred_lft 0
</code></pre>

<p>Repeat for the <code>uplink1</code> interface:</p>

<pre><code># (IPv4) Set up the uplink1 transmit tunnel:
edge# ip link add name fou1v4 type ipip remote 203.0.113.1 local 192.168.178.11 encap fou encap-sport auto encap-dport 1714 dev uplink1
edge# ip link set dev fou1v4 up
edge# ip -4 address add 10.171.0.1/24 dev fou1v4

# (IPv6) Set up the uplink1 transmit tunnel:
edge# ip link add name fou1v6 type sit remote 203.0.113.1 local 192.168.178.11 encap fou encap-sport auto encap-dport 1716 dev uplink1
edge# ip link set dev fou1v6 up
edge# ip -6 address add fd00::10:171:0:1/112 dev fou1v6 preferred_lft 0
</code></pre>

<h3 id="load-balancing-setup">Load-balancing setup</h3>

<p>In previous years, we experimented with setups using MLVPN for load-balancing
traffic on layer 2 across multiple uplinks. Unfortunately, we weren’t able to
get good results: when aggregating links, bandwidth would be limited to the
slowest link. I expect that MLVPN and others would work better this year, if we
were to set it up directly before and after the WiFi uplinks, as the two links
should be almost identical in terms of latency and throughput.</p>

<p>Regardless, we didn’t want to take any chances and decided to go with IP flow
based load-balancing. The downside is that any individual connection can never
be faster than the uplink over which it is routed. Given the number of
concurrent connections in a typical network, in practice we observed good
utilization of both links regardless.</p>

<p>Let’s tell iptables to mark packets coming from the LAN with one of two values
based on the hash of their source IP, source port, destination IP and
destination port properties:</p>

<pre><code>edge# iptables -t mangle -A PREROUTING -s 10.17.0.0/24 -j HMARK --hmark-tuple src,sport,dst,dport --hmark-mod 2 --hmark-offset 10 --hmark-rnd 0xdeadbeef
</code></pre>

<p>Note that the <code>--hmark-offset</code> parameter is required: mark 0 is the default, so
you need an offset of at least 1.</p>

<p>For debugging, it is helpful to exempt the IP addresses we use on the tunnels
themselves, otherwise we might not be able to ping an endpoint which is actually
reachable:</p>

<pre><code>edge# iptables -t mangle -A PREROUTING -s 10.17.0.0/24 -d 10.170.0.0/24 -m comment --comment &quot;for debugging&quot; -j MARK --set-mark 10
edge# iptables -t mangle -A PREROUTING -s 10.17.0.0/24 -d 10.171.0.0/24 -m comment --comment &quot;for debugging&quot; -j MARK --set-mark 11
</code></pre>

<p>Now, we need to add a routing policy to select the correct default route based
on the firewall mark:</p>

<pre><code>edge# ip -4 rule add fwmark 10 table 10
edge# ip -4 rule add fwmark 11 table 11
</code></pre>

<p>The steps for IPv6 are identical.</p>

<p>Note that current OpenWrt (15.05) does not provide the HMARK iptables module. I
filed <a href="https://github.com/openwrt/openwrt/issues/572">a GitHub issue with OpenWrt</a>.</p>

<h4 id="connectivity-for-the-edge-router">Connectivity for the edge router</h4>

<p>Because our default routes are placed in table 110 and 111, the router does not
have upstream connectivity. This is mostly working as intended, as it makes it
harder to accidentally route traffic outside of the tunnels.</p>

<p>There is one exception: we need a route to our DNS server:</p>

<pre><code>edge# ip -4 rule add to 8.8.8.8/32 lookup 110
</code></pre>

<p>It doesn’t matter which uplink we use for that, since DNS traffic is tiny.</p>

<h4 id="connectivity-to-the-tunnel-endpoint">Connectivity to the tunnel endpoint</h4>

<p>Of course, the tunnel endpoint itself must also be reachable:</p>

<pre><code>edge# ip rule add fwmark 110 lookup 110
edge# ip rule add fwmark 111 lookup 111

edge# iptables -t mangle -A OUTPUT -d 203.0.113.1/32 -p udp --dport 1704 -j MARK --set-mark 110
edge# iptables -t mangle -A OUTPUT -d 203.0.113.1/32 -p udp --dport 1714 -j MARK --set-mark 111
edge# iptables -t mangle -A OUTPUT -d 203.0.113.1/32 -p udp --dport 1706 -j MARK --set-mark 110
edge# iptables -t mangle -A OUTPUT -d 203.0.113.1/32 -p udp --dport 1716 -j MARK --set-mark 111
</code></pre>

<h4 id="connectivity-to-the-access-points">Connectivity to the access points</h4>

<p>By clearing the firewall mark, we ensure traffic doesn’t get sent through our
tunnel:</p>

<pre><code>edge# iptables -t mangle -A PREROUTING -s 10.17.0.0/24 -d 192.168.178.250 -j MARK --set-mark 0 -m comment --comment &quot;for debugging&quot;
edge# iptables -t mangle -A PREROUTING -s 10.17.0.0/24 -d 192.168.178.251 -j MARK --set-mark 0 -m comment --comment &quot;for debugging&quot;
edge# iptables -t mangle -A PREROUTING -s 10.17.0.0/24 -d 192.168.178.252 -j MARK --set-mark 0 -m comment --comment &quot;for debugging&quot;
edge# iptables -t mangle -A PREROUTING -s 10.17.0.0/24 -d 192.168.178.253 -j MARK --set-mark 0 -m comment --comment &quot;for debugging&quot;
</code></pre>

<p>Also, since the access points are all in the same subnet, we need to tell Linux
on which interface to send the packets, otherwise packets might egress on the
wrong link:</p>

<pre><code>edge# ip -4 route add 192.168.178.252 dev uplink0 src 192.168.178.10
edge# ip -4 route add 192.168.178.253 dev uplink1 src 192.168.178.11
</code></pre>

<h4 id="mtu-configuration">MTU configuration</h4>

<pre><code>edge# ifconfig uplink0 mtu 1472
edge# ifconfig uplink1 mtu 1472
edge# ifconfig fou0v4 mtu 1416
edge# ifconfig fou0v6 mtu 1416
edge# ifconfig fou1v4 mtu 1416
edge# ifconfig fou1v6 mtu 1416
</code></pre>

<h4 id="for-maintenance-temporarily-use-only-one-uplink">For maintenance: temporarily use only one uplink</h4>

<p>It might come in handy to quickly be able to disable an uplink, be it for
diagnosing issues, performing maintenance on a link, or to work around a broken
uplink.</p>

<p>Let’s create a separate iptables chain in which we can place temporary
overrides:</p>

<pre><code>edge# iptables -t mangle -N prerouting_override
edge# iptables -t mangle -A PREROUTING -j prerouting_override
edge# ip6tables -t mangle -N prerouting_override
edge# ip6tables -t mangle -A PREROUTING -j prerouting_override
</code></pre>

<p>With the following shell script, we can then install such an override:</p>

<pre><code>#!/bin/bash
# vim:ts=4:sw=4
# enforces using a single uplink
# syntax:
#	./uplink.sh 0  # use only uplink0
#	./uplink.sh 1  # use only uplink1
#	./uplink.sh    # use both uplinks again

if [ &quot;$1&quot; = &quot;0&quot; ]; then
	# Use only uplink0
	MARK=10
elif [ &quot;$1&quot; = &quot;1&quot; ]; then
	# Use only uplink1
	MARK=11
else
	# Use both uplinks again
	iptables -t mangle -F prerouting_override
	ip6tables -t mangle -F prerouting_override
	ip -4 rule del to 8.8.8.8/32
	ip -4 rule add to 8.8.8.8/32 lookup &quot;110&quot;
	exit 0
fi

iptables -t mangle -F prerouting_override
iptables -t mangle -A prerouting_override -s 10.17.0.0/24 -j MARK --set-mark &quot;${MARK}&quot;
ip6tables -t mangle -F prerouting_override
ip6tables -t mangle -A prerouting_override -j MARK --set-mark &quot;${MARK}&quot;

ip -4 rule del to 8.8.8.8/32
ip -4 rule add to 8.8.8.8/32 lookup &quot;1${MARK}&quot;
</code></pre>

<h3 id="mss-clamping">MSS clamping</h3>

<p>Because Path MTU discovery is often broken on the internet, it’s best practice
to limit the Maximum Segment Size (MSS) of each TCP connection, achieving the
same effect (but only for TCP connections).</p>

<p>This technique is called “MSS clamping”, and can be implemented in Linux like
so:</p>

<pre><code>edge# iptables -t mangle -A POSTROUTING -p tcp --tcp-flags SYN,RST SYN -o fou0v4 -j TCPMSS --clamp-mss-to-pmtu
edge# iptables -t mangle -A POSTROUTING -p tcp --tcp-flags SYN,RST SYN -o fou1v4 -j TCPMSS --clamp-mss-to-pmtu
edge# ip6tables -t mangle -A POSTROUTING -p tcp --tcp-flags SYN,RST SYN -o fou0v6 -j TCPMSS --clamp-mss-to-pmtu
edge# ip6tables -t mangle -A POSTROUTING -p tcp --tcp-flags SYN,RST SYN -o fou1v6 -j TCPMSS --clamp-mss-to-pmtu
</code></pre>

<h3 id="traffic-shaping">Traffic shaping</h3>

<h4 id="shaping-upstream">Shaping upstream</h4>

<p>With asymmetric internet connections, such as the 400/20 cable connection we’re
using, it’s necessary to shape traffic such that the upstream is never entirely
saturated, otherwise the TCP ACK packets won’t reach their destination in time
to saturate the downstream.</p>

<p>While the FritzBox might already provide traffic shaping, we wanted to
voluntarily restrict our upstream usage to leave some headroom for my parents.</p>

<p>Hence, we’re shaping each uplink to 8 Mbit/s, which sums up to 16 Mbit/s, well
below the available 20 Mbit/s:</p>

<pre><code>edge# tc qdisc replace dev uplink0 root tbf rate 8mbit latency 50ms burst 4000
edge# tc qdisc replace dev uplink1 root tbf rate 8mbit latency 50ms burst 4000
</code></pre>

<p>The specified <code>latency</code> value is a best guess, and the <code>burst</code> value is derived
from the kernel internal timer frequency (<code>CONFIG_HZ</code>) (!), packet size and rate
as per
<a href="https://unix.stackexchange.com/questions/100785/bucket-size-in-tbf">https://unix.stackexchange.com/questions/100785/bucket-size-in-tbf</a>.</p>

<p>Tip: keep in mind to disable shaping temporarily when you’re doing bandwidth
tests ;-).</p>

<h4 id="shaping-downstream">Shaping downstream</h4>

<p>It’s somewhat of a mystery to me why this helped, but we achieved noticeably
better bandwidth (50 Mbit/s without, 100 Mbit/s with shaping) when we also
shaped the downstream traffic (i.e. made the tunnel endpoint shape traffic).</p>

<h3 id="lan">LAN</h3>

<p>For DHCP, DNS and IPv6 router advertisments, we set up
<a href="https://manpages.debian.org/stretch/dnsmasq-base/dnsmasq.8.en.html"><code>dnsmasq(8)</code></a>,
which worked beautifully and was way quicker to configure than the bigger ISC
servers:</p>

<pre><code>edge# apt install dnsmasq
edge# cat &gt; /etc/dnsmasq.d/rgb2r &lt;&lt;'EOT'
interface=lan0
dhcp-range=10.17.0.10,10.17.0.250,30m
dhcp-range=::,constructor:lan0,ra-only
enable-ra
cache-size=10000
EOT
</code></pre>

<h3 id="monitoring">Monitoring</h3>

<p>First, install and start Prometheus:</p>

<pre><code>edge# apt install prometheus prometheus-node-exporter prometheus-blackbox-exporter
edge# setcap CAP_NET_RAW=ep /usr/bin/prometheus-blackbox-exporter
edge# systemctl enable --now prometheus prometheus-node-exporter prometheus-blackbox-exporter
</code></pre>

<p>Then, install and start Grafana:</p>

<pre><code>edge# apt install apt-transport-https
edge# wget -qO- https://packagecloud.io/gpg.key | apt-key add -
edge# echo deb https://packagecloud.io/grafana/stable/debian/ stretch main &gt; /etc/apt/sources.list.d/grafana.list
edge# apt update
edge# apt install grafana
edge# systemctl enable --now grafana-server
</code></pre>

<p>Also, install the excellent
<a href="https://grafana.com/plugins/jdbranham-diagram-panel">Diagram</a> Grafana plugin:</p>

<pre><code>edge# grafana-cli plugins install jdbranham-diagram-panel
edge# systemctl restart grafana-server
</code></pre>

<h3 id="config-files">Config files</h3>

<p>I realize this post contains a lot of configuration excerpts which might be hard
to put together. So, you can <a href="http://code.stapelberg.de/git/rgb2rv17-network-setup/">find all the config files in a git
repository</a>. As I
mentioned at the beginning of the article, please create your own network and
don’t expect the config files to just work out of the box.</p>

<h3 id="statistics">Statistics</h3>

<ul>
<li><p>We peaked at about 60 active DHCP leases.</p></li>

<li><p>The connection tracking table (holding an entry for each IPv4 connection)
never exceeded 4000 connections.</p></li>

<li><p>DNS traffic peaked at about 12 queries/second.</p></li>

<li><p>dnsmasq’s maximum cache size of 10000 records was sufficient: we did not have
a single cache eviction over the entire event.</p></li>

<li><p>We were able to obtain peaks of over 150 Mbit/s of download traffic.</p></li>

<li><p>At peak, about 10% of our traffic was IPv6.</p></li>
</ul>

<h3 id="wifi-statistics">WiFi statistics</h3>

<ul>
<li><p>On link 1, our signal to noise ratio hovered between 31 dBm to 33 dBm. When it
started raining, it dropped by 2-3 dBm.</p></li>

<li><p>On link 2, our signal to noise ratio hovered between 34 dBm to 36 dBm. When it
started raining, it dropped by 1 dBm.</p></li>
</ul>

<p>Despite the relatively bad signal/noise ratios, we could easily obtain about 140
Mbps on the WiFi layer, which results in 100 Mbps on the ethernet layer.</p>

<p>The difference in signal/noise ratio between the two links had no visible impact
on bandwidth, but ICMP probes showed measurably more packet loss on link 1.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Making GitLab authenticate against dex]]></title>
    <link href="https://michael.stapelberg.de/posts/2017-10-21-gitlab-with-dex/"/>
    <id>https://michael.stapelberg.de/posts/2017-10-21-gitlab-with-dex/</id>
    <published>2017-10-21T15:19:00+02:00</published>
    <updated>2017-10-21T15:19:00+02:00</updated>
    <content type="html"><![CDATA[

<p>Because I found it frustratingly hard to make GitLab and dex talk to each other,
this article walks you through what I did step-by-step.</p>

<p>Let’s establish some terminology:</p>

<ul>
<li><p><a href="https://github.com/coreos/dex">dex</a> is our OpenID Connect (OIDC) “Provider
(OP)”<br>in other words: the component which verifies usernames and passwords.</p></li>

<li><p><a href="https://gitlab.com/">GitLab</a> is our OpenID Connect (OIDC) “Relying Party
(RP)”<br>in other words: the component where the user actually wants to log
in.</p></li>
</ul>

<h3 id="step-1-configure-dex">Step 1: configure dex</h3>

<p>First, I followed dex’s <a href="https://github.com/coreos/dex/blob/master/Documentation/getting-started.md">Getting
started</a>
guide until I had dex serving the example config.</p>

<p>Then, I made the following changes to
<a href="https://github.com/coreos/dex/blob/master/examples/config-dev.yaml">examples/config-dev.yaml</a>:</p>

<ol>
<li>Change the issuer URL to be fully qualified and use HTTPS.</li>
<li>Configure the HTTPS listener.</li>
<li>Configure GitLab’s redirect URI.</li>
</ol>

<p>Here is a diff:</p>

<pre><code class="language-diff">--- /proc/self/fd/11	2017-10-21 15:01:49.005587935 +0200
+++ /tmp/config-dev.yaml	2017-10-21 15:01:47.121632025 +0200
@@ -1,7 +1,7 @@
 # The base path of dex and the external name of the OpenID Connect service.
 # This is the canonical URL that all clients MUST use to refer to dex. If a
 # path is provided, dex's HTTP service will listen at a non-root URL.
-issuer: http://127.0.0.1:5556/dex
+issuer: https://dex.example.net:5554/dex
 
 # The storage configuration determines where dex stores its state. Supported
 # options include SQL flavors and Kubernetes third party resources.
@@ -14,11 +14,9 @@
 
 # Configuration for the HTTP endpoints.
 web:
-  http: 0.0.0.0:5556
-  # Uncomment for HTTPS options.
-  # https: 127.0.0.1:5554
-  # tlsCert: /etc/dex/tls.crt
-  # tlsKey: /etc/dex/tls.key
+  https: dex.example.net:5554
+  tlsCert: /etc/letsencrypt/live/dex.example.net/fullchain.pem
+  tlsKey: /etc/letsencrypt/live/dex.example.net/privkey.pem
 
 # Uncomment this block to enable the gRPC API. This values MUST be different
 # from the HTTP endpoints.
@@ -50,7 +48,7 @@
 staticClients:
 - id: example-app
   redirectURIs:
-  - 'http://127.0.0.1:5555/callback'
+  - 'http://gitlab.example.net/users/auth/mydex/callback'
   name: 'Example App'
   secret: ZXhhbXBsZS1hcHAtc2VjcmV0
</code></pre>

<h3 id="step-2-configure-gitlab">Step 2: configure GitLab</h3>

<p>First, I followed <a href="https://docs.gitlab.com/omnibus/docker/">GitLab Docker
images</a> to get GitLab running in
Docker.</p>

<p>Then, I swapped out the image with
<a href="https://hub.docker.com/r/computersciencehouse/gitlab-ce-oidc/">computersciencehouse/gitlab-ce-oidc</a>,
which is based on the official image, but adds OpenID Connect support.</p>

<p>I added the following config to <code>/srv/gitlab/config/gitlab.rb</code>:</p>

<pre><code>gitlab_rails['omniauth_enabled'] = true

# Must match the args.name (!) of our configured omniauth provider:
gitlab_rails['omniauth_allow_single_sign_on'] = ['mydex']

# By default, third-party authentication results in a newly created
# user which needs to be unblocked by an admin. Disable this
# additional safety mechanism and directly create users:
gitlab_rails['omniauth_block_auto_created_users'] = false

gitlab_rails['omniauth_providers'] = [
  {
    name: 'openid_connect',  # identifies the omniauth gem to use
    label: 'OIDC',
    args: {
      # The name shows up in the GitLab UI in title-case, i.e. “Mydex”,
      # and must match the name in client_options.redirect_uri below
      # and omniauth_allow_single_sign_on above.
      #
      # NOTE that if you change the name after users have already
      # signed up through the provider, you will need to update the
      # “identities” PostgreSQL table accordingly:
      # echo &quot;UPDATE identities SET provider = 'newdex' WHERE \
      #   provider = 'mydex';&quot; | gitlab-psql gitlabhq_production
      'name':          'mydex',

      # Scope must contain “email”.
      'scope':         ['openid', 'profile', 'email'],

      # Discover all endpoints from the issuer, specifically from
      # https://dex.example.net:5554/dex/.well-known/openid-configuration
      'discovery':     true,

      # Must match the issuer configured in dex:
      # Note that http:// URLs did not work in my tests; use https://
      'issuer':        'https://dex.example.net:5554/dex',

      'client_options': {
        # identifier, secret and redirect_uri must match a
	# configured client in dex.
        'identifier':   'example-app',
        'secret':       'ZXhhbXBsZS1hcHAtc2VjcmV0',
        'redirect_uri': 'https://gitlab.example.net/users/auth/mydex/callback'
      }
    }
  }
]

</code></pre>

<h3 id="step-3-patch-omniauth-openid-connect">Step 3: patch omniauth-openid-connect</h3>

<p>Until <a href="https://github.com/coreos/dex/issues/376">dex issue #376</a> is fixed, the
following patch for the omniauth-openid-connect gem is required:</p>

<pre><code class="language-diff">--- /opt/gitlab/embedded/lib/ruby/gems/2.3.0/gems/omniauth-openid-connect-0.2.3/lib/omniauth/strategies/openid_connect.rb.orig	2017-10-21 12:31:50.777602847 +0000
+++ /opt/gitlab/embedded/lib/ruby/gems/2.3.0/gems/omniauth-openid-connect-0.2.3/lib/omniauth/strategies/openid_connect.rb	2017-10-21 12:34:20.063308560 +0000
@@ -42,24 +42,13 @@
       option :send_nonce, true
       option :client_auth_method
 
-      uid { user_info.sub }
-
+      uid { @email }
       info do
-        {
-          name: user_info.name,
-          email: user_info.email,
-          nickname: user_info.preferred_username,
-          first_name: user_info.given_name,
-          last_name: user_info.family_name,
-          gender: user_info.gender,
-          image: user_info.picture,
-          phone: user_info.phone_number,
-          urls: { website: user_info.website }
-        }
+        { email: @email }
       end
 
       extra do
-        {raw_info: user_info.raw_attributes}
+        {raw_info: {}}
       end
 
       credentials do
@@ -165,6 +154,7 @@
               client_id: client_options.identifier,
               nonce: stored_nonce
           )
+          @email = _id_token.raw_attributes['email']
           _access_token
         }.call()
       end
</code></pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Why Go is my favorite programming language]]></title>
    <link href="https://michael.stapelberg.de/posts/2017-08-19-golang_favorite/"/>
    <id>https://michael.stapelberg.de/posts/2017-08-19-golang_favorite/</id>
    <published>2017-08-19T13:00:00+02:00</published>
    <updated>2017-08-19T13:00:00+02:00</updated>
    <content type="html"><![CDATA[

<p>I strive to respect everybody’s personal preferences, so I usually steer clear
of debates about which is the best programming language, text editor or
operating system.</p>

<p>However, recently I was asked a couple of times why I like and use a lot of <a
href="https://golang.org">Go</a>, so here is a coherent article to fill in the
blanks of my ad-hoc in-person ramblings :-).</p>

<h3 id="my-background">My background</h3>

<p>I have used C and Perl for a number of decently sized projects. I have written
programs in Python, Ruby, C++, CHICKEN Scheme, Emacs Lisp, Rust and Java (for
Android only). I understand a bit of Lua, PHP, Erlang and Haskell. In a previous
life, I developed a number of programs using
<a href="https://en.wikipedia.org/wiki/Delphi_(programming_language)">Delphi</a>.</p>

<p>I had a brief look at Go in 2009, when it was first released. I seriously
started using the language when Go 1.0 was released in 2012, featuring the <a href="https://golang.org/doc/go1compat">Go 1
compatibility guarantee</a>. I still have
<a href="https://github.com/stapelberg/greetbot">code</a> running in production which I
authored in 2012, largely untouched.</p>

<h3 id="1-clarity">1. Clarity</h3>

<h4 id="formatting">Formatting</h4>

<p>Go code, by convention, is formatted using the
<a href="https://golang.org/cmd/gofmt/"><code>gofmt</code></a> tool. Programmatically formatting code
is not a new idea, but contrary to its predecessors, <code>gofmt</code> supports precisely
one canonical style.</p>

<p>Having all code formatted the same way makes reading code easier; the code feels
familiar. This helps not only when reading the standard library or Go compiler,
but also when working with many code bases — think Open Source, or big
companies.</p>

<p>Further, auto-formatting is a huge time-saver during code reviews, as it
eliminates an entire dimension in which code could be reviewed before: now, you
can just let your continuous integration system verify that <code>gofmt</code> produces no
diffs.</p>

<p>Interestingly enough, having my editor apply <code>gofmt</code> when saving a file has
changed the way I write code. I used to attempt to match what the formatter
would enforce, then have it correct my mistakes. Nowadays, I express my thought
as quickly as possible and trust <code>gofmt</code> to make it pretty
(<a href="https://play.golang.org/p/I6GJwiT77v">example</a> of what I would type, click
Format).</p>

<h4 id="high-quality-code">High-quality code</h4>

<p>I use the standard library (<a href="https://golang.org/pkg/">docs</a>,
<a href="https://github.com/golang/go/tree/master/src">source</a>) quite a bit, see below.</p>

<p>All standard library code which I have read so far was of extremely high quality.</p>

<p>One example is the <a href="https://golang.org/pkg/image/jpeg/"><code>image/jpeg</code></a> package: I
didn’t know how JPEG worked at the time, but it was easy to pick up by switching
between the <a href="https://en.wikipedia.org/wiki/JPEG">Wikipedia JPEG article</a> and the
<code>image/jpeg</code> code. If the package had a few more comments, I would qualify it as
a teaching implementation.</p>

<h4 id="opinions">Opinions</h4>

<p>I have come to agree with many opinions the Go community holds, such as:</p>

<ul>
<li><a href="https://github.com/golang/go/wiki/CodeReviewComments#variable-names">Variable names</a> should be short by default, and become more descriptive the further from its declaration a name is used.</li>
<li>Keep the dependency tree small (to a reasonable degree): <a href="https://www.youtube.com/watch?v=PAAkCSZUG1c&amp;t=9m28s">a little copying is better than a little dependency</a></li>
<li>There is a cost to introducing an abstraction layer. Go code is usually rather clear, at the cost of being a bit repetitive at times.</li>
<li>See <a href="https://github.com/golang/go/wiki/CodeReviewComments">CodeReviewComments</a> and <a href="https://go-proverbs.github.io/">Go Proverbs</a> for more.</li>
</ul>

<h4 id="few-keywords-and-abstraction-layers">Few keywords and abstraction layers</h4>

<p>The Go specification lists only <a href="https://golang.org/ref/spec#Keywords">25
keywords</a>, which I can easily keep in my
head.</p>

<p>The same is true for <a href="https://golang.org/pkg/builtin/">builtin functions</a> and
<a href="https://golang.org/ref/spec#Types">types</a>.</p>

<p>In my experience, the small number of abstraction layers and concepts makes the
language easy to pick up and quickly feel comfortable in.</p>

<p>While we’re talking about it: I was surprised about how readable the <a href="https://golang.org/ref/spec">Go
specification</a> is. It really seems to target
programmers (rather than standards committees?).</p>

<h3 id="2-speed">2. Speed</h3>

<h4 id="quick-feedback-low-latency">Quick feedback / low latency</h4>

<p>I love quick feedback: I appreciate websites which load quickly, I prefer fluent
User Interfaces which don’t lag, and I will choose a quick tool over a more
powerful tool any day. <a href="https://blog.gigaspaces.com/amazon-found-every-100ms-of-latency-cost-them-1-in-sales/">The
findings</a>
of large web properties confirm that this behavior is shared by many.</p>

<p>The authors of the Go compiler respect my desire for low latency: compilation
speed matters to them, and new optimizations are carefully weighed against
whether they will slow down compilation.</p>

<p>A friend of mine had not used Go before. After installing the
<a href="https://robustirc.net">RobustIRC</a> bridge using <code>go get</code>, they concluded that Go
must be an interpreted language and I had to correct them: no, the Go compiler
just is that fast.</p>

<p>Most Go tools are no exception, e.g. <code>gofmt</code> or <code>goimports</code> are blazingly fast.</p>

<h4 id="maximum-resource-usage">Maximum resource usage</h4>

<p>For batch applications (as opposed to interactive applications), utilizing the
available resources to their fullest is usually more important than low latency.</p>

<p>It is delightfully easy to profile and change a Go program to utilize all
available IOPS, network bandwidth or compute. As an example, I wrote about
<a href="https://people.debian.org/~stapelberg/2014/01/17/debmirror-rackspace.html">filling a 1 Gbps
link</a>,
and optimized <a href="https://github.com/Debian/debiman/">debiman</a> to utilize all
available resources, reducing its runtime by hours.</p>

<h3 id="3-rich-standard-library">3. Rich standard library</h3>

<p>The <a href="https://golang.org/pkg">Go standard library</a> provides means to effectively
use common communications protocols and data storage formats/mechanisms, such as
TCP/IP, HTTP, JPEG, SQL, …</p>

<p>Go’s standard library is the best one I have ever seen. I perceive it as
well-organized, clean, small, yet comprehensive: I often find it possible to
write reasonably sized programs with just the standard library, plus one or two
external packages.</p>

<p>Domain-specific data types and algorithms are (in general) not included and live
outside the standard library,
e.g. <a href="https://godoc.org/golang.org/x/net/html"><code>golang.org/x/net/html</code></a>. The
<code>golang.org/x</code> namespace also serves as a staging area for new code before it
enters the standard library: the Go 1 compatibility guarantee precludes any
breaking changes, even if they are clearly worthwhile. A prominent example is
<code>golang.org/x/crypto/ssh</code>, which had to break existing code to <a href="https://github.com/golang/crypto/commit/e4e2799dd7aab89f583e1d898300d96367750991">establish a more
secure
default</a>.</p>

<h3 id="4-tooling">4. Tooling</h3>

<p>To download, compile, install and update Go packages, I use the <code>go get</code> tool.</p>

<p>All Go code bases I have worked with use the built-in
<a href="https://golang.org/pkg/testing/"><code>testing</code></a> facilities. This results not only
in easy and fast testing, but also in <a href="https://blog.golang.org/cover">coverage
reports</a> being readily available.</p>

<p>Whenever a program uses more resources than expected, I fire up <code>pprof</code>. See
this <a href="https://blog.golang.org/profiling-go-programs">golang.org blog post about
<code>pprof</code></a> for an introduction, or
<a href="https://people.debian.org/~stapelberg/2014/12/23/code-search-taming-the-latency-tail.html">my blog post about optimizing Debian Code
Search</a>. After
importing the <a href="https://golang.org/pkg/net/http/pprof/"><code>net/http/pprof</code>
package</a>, you can profile your server
while it’s running, without recompilation or restarting.</p>

<p>Cross-compilation is as easy as setting the <code>GOARCH</code> environment variable,
e.g. <code>GOARCH=arm64</code> for targeting the Raspberry Pi 3. Notably, tools just work
cross-platform, too! For example, I can profile <a href="https://gokrazy.org">gokrazy</a>
from my amd64 computer: <code>go tool pprof ~/go/bin/linux_arm64/dhcp
http://gokrazy:3112/debug/pprof/heap</code>.</p>

<p><a href="https://godoc.org/golang.org/x/tools/cmd/godoc">godoc</a> displays documentation
as plain text or serves it via HTTP. <a href="https://godoc.org">godoc.org</a> is a public
instance, but I run a local one to use while offline or for not yet published
packages.</p>

<p>Note that these are standard tools coming with the language. Coming from C, each
of the above would be a significant feat to accomplish. In Go, we take them for
granted.</p>

<h3 id="getting-started">Getting started</h3>

<p>Hopefully I was able to convey why I’m happy working with Go.</p>

<p>If you’re interested in getting started with Go, check out <a href="https://github.com/gopheracademy/gopher/blob/1cdbcd9fc3ba58efd628d4a6a552befc8e3912be/bot/bot.go#L516">the beginner’s
resources</a>
we point people to when they join the Gophers slack channel. See
<a href="https://golang.org/help/">https://golang.org/help/</a>.</p>

<h3 id="caveats">Caveats</h3>

<p>Of course, no programming tool is entirely free of problems. Given that this
article explains why Go is my favorite programming language, it focuses on the
positives. I will mention a few issues in passing, though:</p>

<ul>
<li>If you use Go packages which don’t offer a stable API, you might want to use a specific, known-working version. Your best bet is the <a href="https://github.com/golang/dep">dep</a> tool, which is not part of the language at the time of writing.</li>
<li>Idiomatic Go code does not necessarily translate to the highest performance machine code, and the runtime comes at a (small) cost. In the rare cases where I found performance lacking, I successfully resorted to <a href="https://golang.org/cmd/cgo/">cgo</a> or assembler. If your domain is hard-realtime applications or otherwise extremely performance-critical code, your mileage may vary, though.</li>
<li>I wrote that the Go standard library is the best I have ever seen, but that doesn’t mean it doesn’t have any problems. One example is <a href="https://golang.org/issues/20744">complicated handling of comments</a> when modifying Go code programmatically via one of the standard library’s oldest packages, <code>go/ast</code>.</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Auto-opening portal pages with NetworkManager]]></title>
    <link href="https://michael.stapelberg.de/posts/2017-05-25-network-manager-portal/"/>
    <id>https://michael.stapelberg.de/posts/2017-05-25-network-manager-portal/</id>
    <published>2017-05-25T11:37:17+02:00</published>
    <updated>2017-05-25T11:37:17+02:00</updated>
    <content type="html"><![CDATA[<p>
Modern desktop environments like GNOME offer UI for this, but if you’re using a
more bare-bones window manager, you’re on your own. This article outlines how
to get a login page opened in your browser when you’re behind a portal.
</p>

<p>
If your distribution does not automatically enable it (Fedora does, Debian
doesn’t), you’ll first need to enable connectivity checking in NetworkManager:
</p>

<pre>
# cat &gt;&gt; /etc/NetworkManager/NetworkManager.conf &lt;&lt;'EOT'
[connectivity]
uri=http://network-test.debian.org/nm
EOT
</pre>

<p>
Then, add a dispatcher hook which will open a browser when NetworkManager
detects you’re behind a portal. Note that the username must be hard-coded
because the hook runs as root, so this hook will not work as-is on multi-user
systems. The URL I’m using is an always-http URL, also used by Android (I
expect it to be somewhat stable). Portals will redirect you to their login page
when you open this URL.
</p>

<pre>
# cat &gt; /etc/NetworkManager/dispatcher.d/99portal &lt;&lt;EOT
#!/bin/bash

[ "$CONNECTIVITY_STATE" = "PORTAL" ] || exit 0

USER=michael
USERHOME=$(eval echo "~$USER")
export XAUTHORITY="$USERHOME/.Xauthority"
export DISPLAY=":0"
su $USER -c "x-www-browser http://www.gstatic.com/generate_204"
EOT
</pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[HomeMatic re-implementation]]></title>
    <link href="https://michael.stapelberg.de/posts/2017-04-16-homematic-reimplementation/"/>
    <id>https://michael.stapelberg.de/posts/2017-04-16-homematic-reimplementation/</id>
    <published>2017-04-16T10:20:00+00:00</published>
    <updated>2017-04-16T10:20:00+00:00</updated>
    <content type="html"><![CDATA[<p>
A while ago, I got myself a bunch of HomeMatic home automation gear (valve drives, temperature and humidity sensors, power switches). The gear itself works reasonably well, but I found the management software painfully lacking. Hence, I re-implemented my own management software. In this article, I’ll describe my method, in the hope that others will pick up a few nifty tricks to make future re-implementation projects easier.
</p>

<h3>Motivation</h3>

<p>
When buying my HomeMatic devices, I decided to use the wide-spread <a href="http://www.eq-3.com/products/homematic/control-units-and-gateways/homematic-central-control-unit-ccu2.html">HomeMatic Central Control Unit (CCU2)</a>. This embedded device runs the proprietary <code>rfd</code> wireless daemon, which offers an XML-RPC interface, used by the web interface.
</p>

<p>
I find the CCU2’s web interface really unpleasant. It doesn’t look modern, it takes ages to load, and it doesn’t indicate progress. I frequently find myself clicking on a button, only to realize that my previous click was still not processed entirely, and then my current click ends up on a different element that I intended to click. Ugh.
</p>

<p>
More importantly, even if you avoid the CCU2’s web interface altogether and only want to extract sensor values, you’ll come to realize that the device crashes every few weeks. Due to memory pressure, the <code>rfd</code> is killed and doesn’t come back. As a band-aid, I wrote a watchdog cronjob which would just reboot the device. I also reported the bug to the vendor, but never got a reply.
</p>

<p>
When I tried to update the software to a more recent version, things went so wrong that I decided to downgrade and not touch the device anymore. This is not a good state to be in, so eventually I started my project to replace the device entirely. The replacement is <a href="https://github.com/stapelberg/hmgo">hmgo</a>, a central control unit implemented in Go, deployed to a Raspberry Pi running <a href="https://gokrazy.github.io/">gokrazy</a>. The radio module I’m using is HomeMatic’s <a href="https://www.elv.de/homematic-funkmodul-fuer-raspberry-pi-bausatz.html">HM-MOD-RPI-PCB</a>, which is connected to a serial port, much like in the CCU2 itself.
</p>

<h3>Preparation: gather and visualize traces</h3>

<p>
In order to compare the behavior of the CCU2 stock firmware against my software, I wanted to capture some traces. Looking at what goes on over the air (or on the wire) is also a good learning opportunity to understand the protocol.
</p>

<ol>
<li>I wrote a <a href="https://www.wireshark.org">Wireshark</a> dissector (see <a href="https://github.com/stapelberg/hmgo/blob/master/contrib/wireshark/homematic.lua">contrib/homematic.lua</a>). It is a quick &amp; dirty hack, does not properly dissect everything, but it works for the majority of packets. This step alone will make the upcoming work so much easier, because you won’t need to decode packets in your head (and make mistakes!) so often.</li>
<li>I captured traffic from the working system. Conveniently, the CCU2 allows SSH'ing in as <code>root</code> after setting a password. Once logged in, I used <code>lsof</code> and <code>ls /proc/$(pidof rfd)/fd</code> to identify the file descriptors which <code>rfd</code> uses to talk to the serial port. Then, I used <code>strace -e read=7,write=7 -f -p $(pidof rfd)</code> to get hex dumps of each read/write. These hex dumps can directly be fed into <code>text2pcap</code> and can be analyzed with Wireshark.</li>
<li>I also wrote a little Perl script to extract and convert packet hex dumps from homegear debug logs to text2pcap-compatible format. More on that in a bit.</li>
</ol>

<h3>Preparation: research</h3>

<p>
Then, I gathered as much material as possible. I found and ended up using the following resources (in order of frequency):
</p>
<ol>
<li><a href="https://github.com/Homegear/Homegear">homegear source</a></li>
<li><a href="https://svn.fhem.de/">FHEM source</a></li>
<li><a href="https://media.ccc.de/v/30C3_-_5444_-_en_-_saal_g_-_201312301600_-_attacking_homematic_-_sathya_-_malli">homegear presentation</a></li>
<li><a href="https://git.zerfleddert.de/cgi-bin/gitweb.cgi/hmcfgusb">hmcfgusb source</a></li>
<li><a href="https://wiki.fhem.de/wiki/Hauptseite">FHEM wiki</a></li>
</ol>

<h3>Preparation: lab setup</h3>

<p>
Next, I got the hardware to work with a known-good software. I set up homegear on a Raspberry Pi, which took a few hours of compilation time because there were no pre-built Debian stretch arm64 binaries. This step established that the hardware itself was working fine.
</p>

<p>
Also, I got myself another set of traces from homegear, which is always useful.
</p>

<h3>Implementation</h3>

<p>
Now the actual implementation can begin. Note that up until this point, I hadn’t written a single line of actual program code. I defined a few milestones which I wanted to reach:
</p>

<ol>
<li>Talk to the serial port.</li>
<li>Successfully initialize the HM-MOD-RPI-PCB</li>
<li>Receive any BidCoS broadcast packet</li>
<li>Decode any BidCoS broadcast packet (can largely be done in a unit test)</li>
<li>Talk to an already-paired device (re-using the address/key from my homegear setup)</li>
<li>Configure an already-paired device</li>
<li>Pair a device</li>
</ol>

<p>
To make the implementation process more convenient, I changed the compilation command of my editor to cross-compile the program, <code>scp</code> it to the Raspberry Pi and run it there. This allowed me to test my code with one keyboard shortcut, and I love quick feedback.
</p>

<h3>Retrospective</h3>

<p>
The entire project took a few weeks of my spare time. If I had taken some time off of work, I’m confident I could have implemented it in about a week of full-time work.
</p>

<p>
Consciously doing research, preparation and milestone planning was helpful. It gave me good sense of my progress and achievable goals.
</p>

<p>
As I’ve learnt previously, investing in tools pays off quickly, even for one-off projects like this one. I’d recommend everyone who’s doing protocol-related work to invest some time in learning to use Wireshark and writing custom Wireshark dissectors.
</p>
]]></content>
  </entry>
</feed>
